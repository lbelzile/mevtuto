[
["index.html", "mev package tutorial Preliminary remarks", " mev package tutorial Léo Belzile 2019-06-28 Preliminary remarks The mev package bundles routine for - likelihood-based inference for univariate extremes (including distributions, densities, score, information matrix and higher order asymptotics). The four basic likelihood (generalized extreme value, generalized Pareto, non-homogeneous Poisson process and \\(r\\)-largest) are implemented. For the first two distributions, additional higher order inference tools are available, including profile likelihoods in most parametrizations of interest. Many recent threshold selection diagnostics that have appeared in the literature are implemented. The second includes multivaraite diagnostics (max-stability test, information matrix test, coefficient of tail dependence, extremal coefficient, estimators of the extremal index, extremogram, nonparametric estimation of the angular measure, etc.) Pairwise composite likelihood for general models will be included in a future update. Last, but not least, the package includes functions to simulate from multivariate max-stable vectors in dimension \\(d\\geq 2\\), including extensions of most of the families in the evd package. The package also includes simulation algorithms for \\(R\\)-Pareto processes using accept-reject and composition sampling, and accept-reject for generalized \\(R\\)-Pareto processes. You can install the latest version of the package (v.1.12) directly from CRAN via install.packages(&quot;mev&quot;) "],
["univariate-peaks-over-threshold-analysis.html", "1 Univariate peaks-over-threshold analysis", " 1 Univariate peaks-over-threshold analysis Let \\(\\ell(\\boldsymbol{y}; \\boldsymbol{\\theta})\\) denotes the log-likelihood of an \\(n\\) sample with a \\(p\\)-dimensional parameter \\(\\boldsymbol{\\theta}\\). The score vector is \\(U(\\boldsymbol{\\theta})=\\partial \\ell / \\partial \\boldsymbol{\\theta}\\), while the Fisher information is \\(i(\\boldsymbol{\\theta})=\\mathrm{E}\\{U(\\boldsymbol{\\theta})U(\\boldsymbol{\\theta})^\\top\\}\\). Under regularity conditions, we also have \\(i(\\boldsymbol{\\theta}) = - \\mathrm{E}(\\partial^2 \\ell / \\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}^\\top)\\). The observed information is the Hessian \\(-\\partial^2 \\ell / \\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}^\\top\\), evaluated at the maximum likelihood estimator \\(\\hat{\\boldsymbol{\\theta}}\\). By definition, the maximum likelihood estimator solves the score equation, i.e. \\(U(\\hat{\\boldsymbol{\\theta}})=\\boldsymbol{0}_p\\). If the maximum likelihood estimator is not available in closed-form, this can be used to verify that the optimization routine has converged. It can also be used for Newton methods. There are four basic likelihoods for univariate extremes: the likelihood of the generalized extreme value (GEV) distribution for block maxima, the likelihood for the generalized Pareto distribution and that of the non-homogeneous Poisson process (NHPP) for exceedances above a threshold \\(u\\), the likelihood of the \\(r\\)-largest observations, which can be used to model the \\(r\\)-largest observations per block or threshold exceedances where the threshold is the \\(r\\)th order statistic. The NHPP likelihood includes a contribution for the fraction of points that exceeds the threshold, whereas the generalized Pareto is a conditional distribution, whose normalizing constant \\(\\zeta_u=\\Pr(Y&gt;u)\\) is the third parameter. The generalized extreme value (GEV) distribution with location parameter \\(\\mu \\in \\mathbb{R}\\), scale parameter \\(\\sigma \\in \\mathbb{R}_{+}\\) and shape parameter \\(\\xi \\in \\mathbb{R}\\) is \\[\\begin{align*} G(x) = \\begin{cases} \\exp\\left\\{-\\left(1+\\xi \\frac{x-\\mu}{\\sigma}\\right)^{-1/\\xi}\\right\\}, &amp; \\xi \\neq 0,\\\\ \\exp \\left\\{ -\\exp \\left(-\\frac{x-\\mu}{\\sigma}\\right)\\right\\},&amp; \\xi = 0, \\end{cases} \\end{align*}\\] defined on \\(\\{x \\in \\mathbb{R}: \\xi(x-\\mu)/\\sigma &gt; -1\\}\\) where \\(x_{+} = \\max\\{0, x\\}\\). The case \\(\\xi=0\\) is commonly known as the Gumbel distribution. We denote the distribution by \\(\\mathsf{GEV}(\\mu, \\sigma, \\xi)\\). \\end{defn} The generalized Pareto (GP) distribution with scale \\(\\sigma \\in \\mathbb{R}_{+}\\) and shape \\(\\xi \\in \\mathbb{R}\\) is \\[\\begin{align*} G(x) = \\begin{cases} 1-\\left(1+\\xi \\frac{x}{\\sigma}\\right)_{+}^{-1/\\xi}, &amp; \\xi \\neq 0,\\\\ 1- \\exp \\left(-\\frac{x}{\\sigma}\\right),&amp; \\xi = 0. \\end{cases} \\end{align*}\\] The range of the generalized Pareto distribution is \\([0, -\\sigma/\\xi)\\) if \\(\\xi &lt; 0\\) and is \\(\\mathbb{R}_{+}\\) otherwise. We denote the distribution by \\(\\mathsf{GP}(\\sigma, \\xi)\\). Let \\(Y_{(1)} \\geq \\cdots \\geq Y_{(r)}\\) denote the \\(r\\) largest observations from a sample. The likelihood of the limiting distribution of the point process for the \\(r\\)-largest observations is \\[ \\ell(\\mu,\\sigma,\\xi; \\boldsymbol{y}) \\equiv -r\\log(\\sigma) - \\left(1+\\frac{1}{\\xi}\\right)\\sum_{j=1}^r \\log\\left(1 + \\xi\\frac{y_{(j)}-\\mu}{\\sigma}\\right)_{+} - \\left(1 + \\xi\\frac{y_{(r)}-\\mu}{\\sigma}\\right)^{-1/\\xi}_+, \\quad \\mu,\\xi\\in\\mathbb{R}, \\sigma&gt;0. \\] Consider a sample of \\(N\\) observations, of which \\(n_u\\) exceed \\(u\\) and which we denote by \\(y_1, \\ldots, y_{n_u}\\). The likelihood associated to the limiting distribution of threshold exceedances is \\[\\begin{align} L(\\mu, \\sigma, \\xi; \\boldsymbol{y}) = \\exp \\left[ - c \\left\\{1+ \\xi \\left( \\frac{u-\\mu}{\\sigma}\\right)\\right\\}^{-1/\\xi}_{+}\\right] (c\\sigma)^{-n_u}\\prod_{i=1}^{n_u} \\left\\{1+\\xi\\left( \\frac{y_i-\\mu}{\\sigma}\\right)\\right\\}^{-1/\\xi-1}_{+}, \\qquad \\mu, \\xi \\in \\mathbb{R}, \\sigma &gt;0, \\label{eq:ppp_lik} \\end{align}\\] where \\((\\cdot)_{+} = \\max\\{0, \\cdot\\}\\). The quantity \\(c\\) is a tuning parameter whose role is described in 7.5 of Coles (2001). If we take \\(c=N/m\\), the parameters of the point process likelihood correspond to those of the generalized extreme value distribution fitted to blocks of size \\(m\\). 1.0.1 Profile likelihood The profile likelihood \\(\\ell_\\mathrm{p}\\), a function of \\(\\boldsymbol{\\psi}\\) alone, is obtained by maximizing the likelihood pointwise at each fixed value \\(\\boldsymbol{\\psi}=\\boldsymbol{\\psi}_0\\) over the nuisance vector \\(\\boldsymbol{\\lambda}_{\\psi_0}\\), \\[\\begin{align*} \\ell_\\mathrm{p}(\\boldsymbol{\\psi})=\\max_{\\boldsymbol{\\lambda}}\\ell(\\boldsymbol{\\psi}, \\boldsymbol{\\lambda})=\\ell(\\boldsymbol{\\psi}, \\hat{\\boldsymbol{\\lambda}}_{\\boldsymbol{\\psi}}). \\end{align*}\\] The observed profile information function is \\[j_\\mathrm{p}(\\boldsymbol{\\psi}) =-\\frac{\\partial \\ell_\\mathrm{p}(\\boldsymbol{\\psi})}{\\partial \\boldsymbol{\\psi}\\partial \\boldsymbol{\\psi}^\\top} = \\left\\{j^{\\boldsymbol{\\psi\\psi}}(\\boldsymbol{\\psi}, \\hat{\\boldsymbol{\\lambda}}_{\\boldsymbol{\\psi}})\\right\\}^{-1}. \\] The profile likelihood is not a genuine likelihood in the sense that it is not based on the density of a random variable. This example illustrates some of the functions used in peaks-over-threshold analysis based on fitting a generalized Pareto distribution to threshold exceedances. We use the Venezuelian rainfall data, a time series of daily rainfall precipitations at Maiquetia airport in Venezuela, for the purpose of illustration. library(mev) library(lubridate, quietly = TRUE, warn.conflicts = FALSE) data(&quot;maiquetia&quot;) day &lt;- seq.Date(from = as.Date(&quot;1961-01-01&quot;), to = as.Date(&quot;1999-12-31&quot;), by = &quot;day&quot;) # Keep non-zero rainfall, exclude 1999 observations nzrain &lt;- maiquetia[year(day) &lt; 1999 &amp; maiquetia &gt; 0] We will ignore temporal dependence and stationarity, but these should be considered. The first step in our analysis is to choose a threshold. We select candidates based on quantiles and test whether they lead to stable inference. ths &lt;- quantile(nzrain, seq(0.9, 0.99, length.out = 15)) # Threshold selection diagnostics W.diag(xdat = nzrain, model = &quot;nhpp&quot;, plots = c(&quot;WN&quot;,&quot;PS&quot;), u = ths) NC.diag(nzrain, u = ths) Threshold selection diagnostics indicate that the shape is more or less constant over the range of threshold considered. Wadsworth’s white noise sequence yields a very large residual around \\(u=20\\), but the Northrop–Coleman score test yields \\(P\\)-value path above 0.2 for every threshold under consideration. # Fit using maximum likelihood estimation gpdf &lt;- fit.gpd(nzrain, threshold = 20, show = TRUE) ## Method: Grimshaw ## Log-likelihood: -832.629 ## ## Threshold: 20 ## Number Above: 216 ## Proportion Above: 0.0604 ## ## Estimates ## scale shape ## 15.5800 0.1088 ## ## Standard Errors ## scale shape ## 1.60673 0.07785 ## ## Optimization Information ## Convergence: successful The default optimization routine for the generalized Pareto distribution is Grimshaw’s method, which profiles out the likelihood. The method is almost guaranteed to converge. Because of non-regularity, the maximum likelihood estimator for \\(\\xi &lt; -1\\) does not solve the score equation and leads to infinite log-likelihood, hence the maximum returned lies on the boundary. The standard errors are based on the inverse observed information matrix and provided only if \\(\\xi&gt;-1/2\\). We can verify that our maximum likelihood estimate is indeed a maximum by checking if it solves the score equation if \\(\\hat{\\xi}&gt;-1\\). isTRUE(all.equal( gpd.score(gpdf$estimate, dat = gpdf$exceedances), c(0,0), tolerance = 1e-5)) ## [1] TRUE If the sample is small, maximum likelihood estimators are biased for the generalized Pareto distribution (the shape parameter is negatively biased, regardless of the true value for \\(\\xi\\)). Bias correction methods includes the modified score of Firth, but the default method is the implicit correction (subtract), which solves the implicit equation \\[\\begin{align} \\boldsymbol{\\tilde{\\theta}}=\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{b}(\\tilde{\\boldsymbol{\\theta}}). \\label{eq:implbias} \\end{align}\\] The point estimate \\(\\boldsymbol{\\tilde{\\theta}}\\) is obtained numerically as the root of this nonlinear system of equations. In the present case, the sample size is large and hence the first-order correction, derived through asymptotic arguments from the generalized Pareto distribution likelihood, is small. Note that the bias correction requires \\(\\xi &gt; -1/3\\), since it is based on third-order cumulants of the distribution. gpdbcor &lt;- gpd.bcor(dat = gpdf$exceedances, par = gpdf$estimate) #print the differences between MLE and bias-corrected estimates gpdf$estimate - gpdbcor ## scale shape ## 0.1915284 -0.0118876 The package includes some default diagnostic plots (probability-probability plots and quantile-quantile plots), which include approximate confidence intervals based on order statistics. We can also get profile likelihood and profile-based confidence intervals for most quantities of interest (parameters of the generalized Pareto distribution, excepted shortfall, return levels, \\(N\\)-observation maxima mean and quantiles). The example below gives the estimated profile for the median of the centenial maximum distribution conditional on exceeding 15, along with 95% confidence intervals. plot(gpdf, which = 2) #Q-Q plot # Profile of median of maxima of 100 years profile &lt;- gpd.pll(param = &quot;Nquant&quot;, dat = nzrain, threshold = 15, N = length(nzrain)/(1998-1961)*100, q = 0.5) # 95% confidence intervals conf &lt;- confint(profile, print = TRUE) ## Point estimate for the parameter of interest psi: ## Maximum likelihood : 320.622 ## ## Confidence intervals, levels : 0.025 0.975 ## Wald intervals : 118.72 522.524 ## Profile likelihood : 177.214 741.869 References "],
["penultimate-approximation.html", "2 Penultimate approximation", " 2 Penultimate approximation The generalized extreme value (GEV) distribution arises as the non-degenerate limiting distribution of maxima. A key property of the generalized extreme value distribution is max-stability, meaning that the distribution of maxima of \\(T \\in \\mathbb{N}\\) is the same up to a location-scale transform: if \\(F \\sim \\mathsf{GEV}\\), then there exist location and scale constants \\(b \\in \\mathbb{R}\\) and \\(a&gt;0\\) such that \\(F^T(ax+b) = F(x)\\). The parameters of the new distribution are easily derived: if \\(X_i \\stackrel{\\mathrm{iid}}{\\sim}\\mathsf{GEV}(\\mu,\\sigma, \\xi)\\), then \\(\\max\\{X_1, \\ldots, X_T\\} \\sim \\mathsf{GEV}(\\mu_T, \\sigma_T, \\xi)\\) with \\(\\mu_T = \\mu - \\sigma(1-T^\\xi)/\\xi\\) and \\(\\sigma_T = \\sigma T^\\xi\\) for \\(\\xi \\neq 0\\), or \\(\\mu_T = \\mu +\\sigma \\log(T)\\) and \\(\\sigma_T = \\sigma\\) if \\(\\xi=0\\). In practice, the GEV is fitted to data that are partitioned into \\(k\\) blocks of (potentially random) sizes \\(m_i\\). We shall assume that the block sizes are all the same, so that \\(m_i=m\\) for \\(i=1,\\ldots, k\\) and \\(n = km\\). The quality of the generalized extreme value approximation increases with the block size \\(m\\). For fixed block sizes, the estimated shape parameter generally differs from its asymptotic counterpart and this discrepancy usually introduces bias if one extrapolates far beyond the range of the observed data. Let \\(F(x)\\) denote a thrice-differentiable distribution function with endpoint \\(x^*\\) and density \\(f(x)\\) and define \\(s(x)=-F(x)\\log\\{F(x)\\}/f(x)\\). The existence of the limit \\(\\xi_{\\infty} = \\lim_{n \\to \\infty} s&#39;\\{b_n\\}\\) is necessary and sufficient for convergence to an extreme value distribution \\(G\\), \\[\\begin{align*} \\lim_{n \\to \\infty} F^n(a_nx+b_n) = \\exp\\left\\{-(1+\\xi_\\infty x)^{-1/\\xi_\\infty}\\right\\}=G(x). \\end{align*}\\] Smith (1987) shows that, for any \\(x \\in \\{y:1+\\xi_\\infty y &gt;0\\}\\), there exists \\(z\\) such that \\[\\begin{align*} \\frac{-\\log[F\\{v+xs(v)\\}]}{-\\log\\{F(v)\\}} = \\left\\{1+xs&#39;(z)\\right\\}^{-1/s&#39;(z)}, \\qquad v &lt; z &lt; v+xs(v). \\end{align*}\\] For each \\(n \\geq 1\\), setting \\(v=b_n\\) and \\(a_n=s(b_n)\\) yields \\[\\begin{align*} F^n(a_nx+b_n)=\\exp\\left[-\\left\\{1+s&#39;(z)x\\right\\}^{-1/s&#39;(z)}\\right] + \\mathrm{O}(n^{-1}) \\end{align*}\\] for \\(z \\in [\\min(a_nx+b_n, b_n), \\max(a_nx+b_n, b_n)]\\), depending on the support of \\(F\\). The ultimate approximation replaces \\(s&#39;(z)\\) by \\(s&#39;(x^*)=\\xi_{\\infty}\\), whereas Smith (1987) suggests instead suggests taking \\(s&#39;(b_n)\\), which is closer to \\(s&#39;(a_nx+b_n)\\) than is \\(s&#39;(x^*)\\). The maximum likelihood estimate of the shape should change as the threshold or the block size increases. Consider for simplicity yearly maxima arising from blocking \\(m=n_y\\) observations per year. We are interested in the distribution of the maximum of \\(N\\) years and thus the target has roughly shape \\(\\xi_{Nn_y}\\), but our estimates will instead give approximately \\(\\xi_{n_y}\\); an extrapolation error arises from this mismatch between the shape values, which would be constant if the observations were truly max-stable. The curvature of \\(s&#39;\\) determines how stable the estimates of \\(\\xi_t\\) are when the extrapolation window increases. We illustrate the previous discussion by looking at block maximum from a log-normal distribution. library(mev) set.seed(123) x &lt;- seq(1, 30, length=200) fitted &lt;- t(replicate(n = 1000, fit.gev(apply(matrix(rlnorm(30*100), ncol= 30), 1, max), method = &quot;BFGS&quot;)$est)) penult.bm.lnorm &lt;- mev::smith.penult(model = &quot;bm&quot;, m = (m &lt;- 30), family = &quot;lnorm&quot;) par(mfrow = c(1,2), mar = c(5,5,1,1)) hist(fitted[,3], probability = TRUE, breaks = 20, xlab = &quot;Estimated shape&quot;, main = &quot;&quot;) segments(y0 = -0.5, y1 = 0, x0 = penult.bm.lnorm$shape, col = &quot;red&quot;, lwd = 2) segments(y0 = -0.5, y1 = 0, x0 = 0, col = &quot;black&quot;, lwd = 2) p30 &lt;- penult.bm.lnorm x = seq(0,100, length = 400) N &lt;- 1000; N30 = N/30 plot(x, N*exp((N-1)*plnorm(x, log.p=TRUE))* dlnorm(x), type=&quot;l&quot;, bty = &quot;l&quot;, ylim = c(0,0.1), xlab=&quot;x&quot;, ylab=&quot;Density&quot;) # Get parameters of maximum of N GEV maxstabp &lt;- function(loc, scale, shape, N){ if(!isTRUE(all.equal(shape, 0, check.attributes = FALSE))){ mut &lt;- loc - scale*(1-N^shape)/shape sigmat = scale*N^shape return(c(loc = mut, scale = sigmat, shape = shape)) } else{ mut &lt;- loc + scale*log(N) return(c(loc = mut, scale = scale, shape = shape)) } } p30e &lt;- maxstabp(loc = p30$loc, scale = p30$scale, shape = p30$shape, N = N30) lines(x, evd::dgev(x, loc = p30e[&#39;loc&#39;], scale = p30e[&#39;scale&#39;], shape = p30e[&#39;shape&#39;]), col = &quot;red&quot;, lwd = 2, lty = 2) p30l &lt;- maxstabp(loc = p30$loc, scale = p30$scale, shape = 0, N = N30) lines(x, evd::dgev(x, loc = p30e[&#39;loc&#39;], scale = p30l[&#39;scale&#39;], shape = 0), col = 1, lty=3, lwd=1) legend(x = &quot;topright&quot;, legend = c(&quot;exact&quot;,&quot;ultimate&quot;, &quot;penultimate&quot;), col = c(1,1,2), lwd = c(2,2,1), bty = &quot;n&quot;, lty = c(1,3,2)) The left panel illustrates how maximum likelihood estimates of the parameters for repeated samples from a log-normal distribution are closer to the penultimate approximation than to the limiting tail index \\(\\xi_{\\infty}=0\\). Using max-stability, we could get an estimate of the distribution of the maximum of \\(Nn_y\\) observations. In this case, the penultimate shape for \\(m=1000\\) is approximately \\(\\xi_{1000}=0.215\\), compared to \\(\\xi_{30} \\approx 0.284\\), which is far from the limiting value \\(\\xi_{\\infty}=0\\). The extrapolated density estimate is based on \\(F_1^{1000/30}\\), the generalized extreme value density associated to the distribution function of the penultimate approximation \\(F_1^{1000/30}\\), with \\(F_1 \\sim\\mathsf{GEV}(a_{30}, b_{30}, \\xi_{30})\\). The penultimate approximation \\(F_2\\) is more accurate than the ultimate approximation \\(\\mathsf{GEV}(a_{30}, b_{30}, \\xi_\\infty)\\), which is too short tailed. The function smith.penult computes the penultimate approximation for parameter models in R for the distribution assuming a model family with arguments dfamily, qfamily and pfamily exist. It returns the penultimate parameters for a given block size or quantile in case of threshold models. These functions are particularly useful for simulation studies in which one wants to obtain an approximation for \\(F^N\\) for large \\(N\\), to get an approximation to the asymptotic distribution of a test statistic for finite \\(N\\); the \\(\\mathsf{GEV}\\) penultimate approximation is more numerically stable for certain models and its moments and cumulants are easily derived. The following plot illustrates the penultimate shape parameter for the normal distribution based on threshold exceedances. In finite samples, the upper tail is bounded, but the quality of the approximation by the Gumbel distribution increases as \\(u \\to \\infty\\), albeit very slowly. penult &lt;- smith.penult(family = &quot;norm&quot;, method = &quot;pot&quot;, u = qnorm(u &lt;- seq(0.8, 0.9999, by = 0.0001))) plot(u, penult$shape, bty = &quot;l&quot;, type=&#39;l&#39;, xlab=&#39;Quantile&#39;, ylab=&#39;Penultimate shape&#39;) References "],
["threshold-selection.html", "3 Threshold selection 3.1 Robust selection 3.2 Threshold stability plots 3.3 Wadsworth’s diagnostics: white noise process and simultaneous threshold stability plots 3.4 Changepoint tests based on penultimate approximation 3.5 Extended generalized Pareto", " 3 Threshold selection The generalized Pareto (GP) distribution with scale \\(\\sigma \\in \\mathbb{R}_{+}\\) and shape \\(\\xi \\in \\mathbb{R}\\) is \\[\\begin{align*} G(x) = \\begin{cases} 1-\\left\\{1+\\xi \\left(\\frac{x}{\\sigma}\\right)\\right\\}_{+}^{-1/\\xi}, &amp; \\xi \\neq 0, \\\\ 1-\\exp \\left(-{x}{\\sigma}\\right),&amp; \\xi = 0. \\end{cases} \\end{align*}\\] The range of the generalized Pareto distribution is \\([0, -\\sigma/\\xi)\\) if \\(\\xi &lt; 0\\) and \\(\\mathbb{R}_{+}\\) otherwise. Why is the generalized Pareto distribution so central to peaks-over-threshold analysis? The conditional distribution of exceedances over a threshold \\(u &lt; x^*\\) converges to a generalized Pareto distribution as \\(u\\) converges to the endpoint of the distribution, \\[\\begin{align*} \\lim_{u \\to x^*} \\frac{1-F(xa_u+u)}{1-F(u)} = 1-H(x), \\end{align*}\\] where \\(H(x)\\) is the distribution function of \\(\\mathsf{GP}(1, \\xi)\\). If \\(X \\sim \\mathsf{GP}(\\sigma, \\xi)\\), straightforward calculations show that \\(X-u \\mid X&gt;u \\sim \\mathsf{GP}(\\sigma + \\xi u, \\xi)\\) for any \\(u \\in \\mathbb{R}\\) such that \\(\\sigma+\\xi u&gt;0\\), meaning that conditional exceedances above a threshold \\(u\\) also follow a generalized Pareto distribution. This property is termed threshold-stability. The limiting distribution of threshold exceedances is generalized Pareto as \\(u \\to x^*\\) but, in practice, we must choose a finite threshold in order to have enough exceedances to draw inference. Since the scaling constant \\(a_u\\) is unknown, we have \\(X \\mid X &gt; u \\stackrel{\\cdot}{\\sim} \\mathsf{GP}(\\sigma_u, \\xi)\\). The term \\(1-F(u)\\) in the denominator is the fraction of points exceeding the threshold, which has a binomial distribution. Threshold selection is subtle and it is common to select a high percentile of the data, say the 95% value, as the threshold, even if this is asymptotically incorrect, as in this case \\(k/n \\nrightarrow 0\\) as \\(n \\to \\infty\\). Most approaches for threshold selection rely on properties of the generalized Pareto distribution (moments, threshold-stability) to determine a region within which the asymptotic distribution fits the data well and the parameter estimates are stable. Below, we focus on recent graphical selection tools for likelihood based inference; mixture models are reviewed in Scarrott and MacDonald (2012) and are available in the package evmix. 3.1 Robust selection The extreme value distributions have unbounded influence functions and outliers can strongly affect the estimate of the shape. Dupuis (1999) proposes an optimal \\(B\\)-robust estimator of the generalized Pareto parameters. Points that are outlying or for which the fit is poor are downweighted; if the weights for the largest observations are very low, this suggests that the threshold is too low. While there is no guarantee that observations that were simulated from a generalized Pareto distributed would not be downweighted, systematic downweighting of the largest exceedances may be indicative of poor fit. 3.2 Threshold stability plots Consider a sequence of ordered candidate thresholds \\(u_1 &lt; \\cdots &lt; u_k\\); one of the most widely used tools for threshold selection is the threshold-stability plots of Davison and Smith (1990). These show the point estimates of the shape \\(\\xi\\) and the modified scale \\(\\sigma_{u_i}-\\xi u_i\\), which should be constant for any threshold \\(u_{j} &gt;u_i\\) assuming that the generalized Pareto above \\(u_i\\) holds exactly. In addition to the point estimates, the asymptotic pointwise 95% Wald confidence intervals are displayed; the standard errors are obtained from the observed information matrix. While these are displayed by many packages, notably extRemes, ismev and evd , mev allows you to use profile likelihood based confidence intervals, which typically offer better coverage and capture some of the asymmetry of the distribution. The mev package functions for producing threshold stability plots are tstab.gpd and tstab.pp for respectively the generalized Pareto and Poisson process likelihoods. Parameter stability plots can be difficult to interpret because the confidence intervals are pointwise rather than simultaneous (each fit likewise uses an overlapping portion of the data). The plots also ignore changes in the estimated parameters due to the penultimate approximation. 3.3 Wadsworth’s diagnostics: white noise process and simultaneous threshold stability plots The problem with the threshold stability plots lies in the point-wise nature of the estimate. Assuming a superposition of \\(k\\) Poisson processes, Wadsworth (2016) derives the limiting distribution of the maximum likelihood estimators from the Poisson process for overlapping windows as the number of windows \\(k \\to \\infty\\) and \\(n_k \\to \\infty\\). The joint asymptotic Gaussian distribution allows Wadsworth (2016) to propose two additional diagnostics: a white noise sequence of differences in estimates of the shape, standardized to have unit variance. The variables \\(\\xi^*_i=(\\hat{\\xi}_{u_{i+1}}-\\hat{\\xi}_{u_i})/\\{(I^{-1}_{u_{i+1}}-I^{-1}_{u_{i}})_{\\xi,\\xi}^{1/2}\\}\\), where \\(I_{u_{i}}\\) is the Fisher information of the Poisson process likelihood for exceedances above \\(u_i\\), should form a white-noise sequence of independent variables centered around the origin; systematic deviations are indicative of inadequacy. To formally test the hypothesis, a likelihood ratio test can be used assuming a simple alternative, namely a single change point at threshold \\(u_j\\). The null hypothesis is \\(\\mathrm{H}_0: \\xi_i^* \\stackrel{\\mathrm{iid}}{\\sim}\\textsf{No}(0,1)\\) for \\(i=1, \\ldots, k-1\\) against the alternative \\(\\mathrm{H}_a: \\xi^*_i \\sim \\textsf{No}(\\beta, \\sigma) (i=1, \\ldots, j-1)\\) and \\(\\xi^*_i \\sim \\textsf{No}(0,1)\\) for \\(j, \\ldots, k-1\\). This alternative is motivated by results on model misspecification (White 1982), which suggest that the asymptotic distribution may still be Gaussian, but with a different mean and variance. This can be used to automate threshold selection, by picking the smallest threshold for which the \\(P\\)-value is above the level \\(\\alpha\\). The function W.diag returns diagnostics plots (for the likelihood ratio statistic path, the white noise process and threshold stability along with asymptotic simultaneous confidence intervals) for non-homogeneous Poisson process model and for the bivariate exponential and the over a sequence of thresholds, specified using q1, q2 and k. The argument M is a tuning parameter that can be chosen in a way such that the parameters of the non-homogeneous Poisson process likelihood coincide with those of the generalized extreme value distribution for blocks of size \\(m\\); see Coles (2001) to this effect. A main criticism of the proposals of Wadsworth (2016) is their lack of robustness. For the asymptotic result to be approximately valid, the number of thresholds must be large, which implicitly requires large samples for each superposed point process. Moreover, the estimated difference in Fisher information matrices often fails to be positive definite in practice. The procedure is highly sensitive to the choice of \\(k\\). Changing the set of thresholds \\(\\boldsymbol{u}\\) under consideration leads to potentially completely different parameter estimates being chosen by the automated procedure. 3.4 Changepoint tests based on penultimate approximation Let \\(F(x)\\) denote a thrice-differentiable distribution function with endpoint \\(x^*\\) and density \\(f(x)\\). Define the reciprocal hazard function \\(r(x) = \\{1-F(x)\\}/f(x)\\). The existence of the limit \\(\\xi_{\\infty} = \\lim_{n \\to \\infty} s&#39;\\{b_n\\}\\) is necessary and sufficient for convergence to an extreme value distribution and Smith (1987) shows that there exists \\(y\\) such that \\[\\begin{align*} \\frac{1-F\\{u+xr(u)\\}}{1-F(u)} = \\left\\{1+xr&#39;(y)\\right\\}_{+}^{-1/r&#39;(y)}, \\qquad u &lt; y &lt; u+xr(u), \\end{align*}\\] unless \\(r&#39;(x)\\) is constant. The penultimate shape parameter for the generalized Pareto distribution is \\(r&#39;(u)\\), but the true shape parameter lies between \\(r&#39;(u)\\) and \\(\\xi_{\\infty}\\). When we fit the limiting parametric models to finite samples, maximum likelihood estimates of the shape parameter will be closer to their penultimate counterparts than to the limiting value and we can expect them to vary as we increase the threshold. Northrop and Coleman (2014) adapt the idea of Wadsworth and Tawn (2012) and fit a generalized Pareto model with piecewise constant shape to \\(k\\) different thresholds; continuity constraints at the thresholds impose \\(k-1\\) restrictions on scale parameters, so the model only has \\(k+1\\) parameters. A score test can be used to test the hypothesis of equal shape and it only requires evaluation of the model under the null hypothesis that a generalized Pareto distribution is valid above all thresholds. A diagnostic plot is obtained by plotting \\(P\\)-values against threshold. One can then choose to take, e.g., (a) the lowest threshold at which the \\(P\\)-value is non-significant, or (b) the lowest threshold at which the \\(P\\)-values for all higher thresholds are non-significant: under the null hypothesis, there is an \\(\\alpha\\%\\) probability of rejection at any given threshold. The function NC.diag computes the \\(P\\)-value of the score test as a function of the threshold. 3.5 Extended generalized Pareto Papastathopoulos and Tawn (2013) propose three extended generalized Pareto distributions: for example, the third extended generalized Pareto model has distribution function \\(\\{1-(1+\\xi x/\\sigma)^{-1/\\xi}_{+}\\}^{\\kappa}\\) for \\(x &gt;0\\) and \\(\\kappa &gt; 0\\). Each family reduces to the generalized Pareto when the additional parameter \\(\\kappa=1\\) and share the same tail index \\(\\xi\\), the extended generalized Pareto provide more flexibility for modelling departures from the limiting form. Standard parameter stability plots can be used to find a region in which \\(\\kappa \\approx 1\\) and the shape parameter stabilizes. The additional parameter gives flexibility for modelling departures from the limiting distribution and the hope is one can fit to exceedances over lower threshold and increase the number of points to which the distribution is fitted. The stability plots, obtained through tstab.egp, suffer from the same caveats as classical diagnostics. References "],
["simulation.html", "4 Simulation", " 4 Simulation The original goal of the mev package was to implement the algorithm of Dombry, Engelke, and Oesting (2016) in order to perform exact simulation from max-stable vectors. Since then, other algorithms for simulating from standard \\(R\\)-Pareto processes and generalized \\(R\\)-Pareto processes have been implemented. These are based on accept-reject method or composition sampling. Let’s start by simulating from a simple max-stable random vector from the negative logistic model. The margins are unit Frechet, and we can verify this by pooling the observations and fitting a generalized extreme value distribution to the sample. library(mev) set.seed(1234) samp &lt;- rmev(n = 1000, d = 5, param = 0.1, model = &quot;neglog&quot;) fit.gev(c(samp), show = FALSE)$estimate ## loc scale shape ## 0.994592 1.001284 1.035500 We simulate a spatial generalized \\(R\\)-Pareto process. lon &lt;- seq(650, 720, length = 50) lat &lt;- seq(215, 290, length = 50) # Create a grid grid &lt;- expand.grid(lon,lat) coord &lt;- as.matrix(grid) dianiso &lt;- distg(coord, 1.5, 0.5) sgrid &lt;- scale(grid, scale = FALSE) # Specify marginal parameters `loc` and `scale` over grid eta &lt;- 26 + 0.05*sgrid[,1] - 0.16*sgrid[,2] tau &lt;- 9 + 0.05*sgrid[,1] - 0.04*sgrid[,2] # Parameter matrix of Huesler--Reiss # associated to power variogram Lambda &lt;- ((dianiso/30)^0.7)/4 # Simulate generalized max-Pareto field above u=50 set.seed(345) simu1 &lt;- rgparp(n = 1, thresh = 50, shape = 0.1, riskf = &quot;max&quot;, scale = tau, loc = eta, sigma = Lambda, model = &quot;hr&quot;) # The same, but conditional on an exceedance at a site simu2 &lt;- rgparp(n = 1, thresh = 50, shape = 0.1, riskf = &quot;site&quot;, siteindex = 1225, scale = tau, loc = eta, sigma = Lambda, model = &quot;hr&quot;) #Plot the generalized max-Pareto field par(mfrow = c(1,2)) fields::quilt.plot(grid[,1], grid[,2], simu1, nx = 50, ny = 50) SpatialExtremes::swiss(add = TRUE) fields::quilt.plot(grid[,1], grid[,2], simu2, nx = 50, ny = 50) SpatialExtremes::swiss(add = TRUE) # Value at conditioning coordinate should be greater than 50 simu2[1225] ## [1] 84.81661 The code snippet below fits a Brown–Resnick model with power variogram to simulated data from the same model (based on more than one replicate). The parameters are estimated by minimizing the squared distance between empirical cloud of pairwise conditional probability of exceedance and the theoretical one. We include geometric anisotropy in the analysis. lon &lt;- seq(650, 720, length = 10) lat &lt;- seq(215, 290, length = 10) # Create a grid grid &lt;- expand.grid(lon,lat) coord &lt;- as.matrix(grid) dianiso &lt;- distg(coord, 1.5, 0.5) sgrid &lt;- scale(grid, scale = FALSE) # Specify marginal parameters `loc` and `scale` over grid eta &lt;- 26 + 0.05*sgrid[,1] - 0.16*sgrid[,2] tau &lt;- 9 + 0.05*sgrid[,1] - 0.04*sgrid[,2] # Parameter matrix of Huesler--Reiss # associated to power variogram Lambda &lt;- ((dianiso/30)^0.7)/4 # Simulate generalized max-Pareto field above u=50 set.seed(345) simu1 &lt;- rgparp(n = 1000, thresh = 50, shape = 0.1, riskf = &quot;max&quot;, scale = tau, loc = eta, sigma = Lambda, model = &quot;hr&quot;) extdat &lt;- extremo(dat = simu1, margp = 0.9, coord = coord, scale = 1.5, rho = 0.5, plot = TRUE) # Constrained optimization # Minimize distance between extremal coefficient from fitted variogram mindistpvario &lt;- function(par, emp, coord){ alpha &lt;- par[1]; if(!isTRUE(all(alpha &gt; 0, alpha &lt; 2))){return(1e10)} scale &lt;- par[2]; if(scale &lt;= 0){return(1e10)} a &lt;- par[3]; if(a&lt;1){return(1e10)} rho &lt;- par[4]; if(abs(rho) &gt;= pi/2){return(1e10)} semivariomat &lt;- mgp::power.vario(distg(coord, a, rho), alpha = alpha, scale = scale) sum((2*(1-pnorm(sqrt(semivariomat[lower.tri(semivariomat)]/2))) - emp)^2) } # constrained optimization for the parameters hin &lt;- function(par, ...){ c(1.99-par[1], -1e-5 + par[1], -1e-5 + par[2], par[3]-1, pi/2 - par[4], par[4]+pi/2) } opt &lt;- alabama::auglag(par = c(0.5, 30, 1.5, 0.5), hin = hin, control.optim = list(parscale = c(0.5, 30, 1.5, 0.5)), fn = function(par){ mindistpvario(par, emp = extdat[,&#39;prob&#39;], coord = coord)}) ## Min(hin): 0.49999 ## Outer iteration: 1 ## Min(hin): 0.49999 ## par: 0.5 30 1.5 0.5 ## fval = 77.37 ## ## Outer iteration: 2 ## Min(hin): 0.3992261 ## par: 0.668296 74.8862 1.39923 0.556808 ## fval = 6.52 ## ## Outer iteration: 3 ## Min(hin): 0.3992502 ## par: 0.668285 74.8855 1.39925 0.556807 ## fval = 6.52 ## stopifnot(opt$kkt1, opt$kkt2) # Plotting the extremogram in the deformed space distfa &lt;- distg(loc = coord, opt$par[3], opt$par[4]) plot(c(distfa[lower.tri(distfa)]), extdat[,2], pch = 20, col = scales::alpha(1,0.1), yaxs = &quot;i&quot;, xaxs = &quot;i&quot;, bty = &#39;l&#39;, xlab = &quot;distance&quot;, ylab= &quot;cond. prob. of exceedance&quot;, ylim = c(0,1)) lines(x = (distvec &lt;- seq(0,200, length = 1000)), col = 2, lwd = 2, 2*(1-pnorm(sqrt(power.vario(distvec, alpha = opt$par[1], scale = opt$par[2])/2)))) References "],
["references.html", "References", " References "]
]
