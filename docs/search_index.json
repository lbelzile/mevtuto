[
["index.html", "mev package tutorial Preliminary remarks", " mev package tutorial Léo Belzile 2019-06-29 Preliminary remarks The mev package bundles routine for - likelihood-based inference for univariate extremes (including distributions, densities, score, information matrix and higher order asymptotics). The four basic likelihood (generalized extreme value, generalized Pareto, non-homogeneous Poisson process and \\(r\\)-largest) are implemented. For the first two distributions, additional higher order inference tools are available, including profile likelihoods in most parametrizations of interest. Many threshold selection diagnostics that have recently been proposed in the literature are implemented. The second includes multivariate goodness-of-fit diagnostics and tests (max-stability test, information matrix test, coefficient of tail dependence, extremal coefficient, estimators of the extremal index, extremogram, nonparametric estimation of the angular measure, etc.) Pairwise composite likelihood for general models will be included in a future update. Last, but not least, the package includes functions to simulate from multivariate max-stable vectors in dimension \\(d\\geq 2\\), including extensions of most of the families in the evd package. The package also includes simulation algorithms for \\(R\\)-Pareto processes using accept-reject and composition sampling, and accept-reject for generalized \\(R\\)-Pareto processes. You can install the latest version of the package (v.1.12) directly from CRAN via install.packages(&quot;mev&quot;) These notes are copyright by the author: you may not reuse them without permission. "],
["likelihood-based-inference-for-univariate-extremes.html", "1 Likelihood based inference for univariate extremes 1.1 Basic theory 1.2 Numerical example 1.3 Your turn", " 1 Likelihood based inference for univariate extremes 1.1 Basic theory Let \\(\\ell(\\boldsymbol{y}; \\boldsymbol{\\theta})\\) denotes the log-likelihood of an \\(n\\) sample with a \\(p\\)-dimensional parameter \\(\\boldsymbol{\\theta}\\). The score vector is \\(U(\\boldsymbol{\\theta})=\\partial \\ell / \\partial \\boldsymbol{\\theta}\\), while the Fisher information is \\(i(\\boldsymbol{\\theta})=\\mathrm{E}\\{U(\\boldsymbol{\\theta})U(\\boldsymbol{\\theta})^\\top\\}\\). Under regularity conditions, we also have \\(i(\\boldsymbol{\\theta}) = - \\mathrm{E}(\\partial^2 \\ell / \\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}^\\top)\\). The observed information is the negative Hessian \\(-\\partial^2 \\ell / \\partial \\boldsymbol{\\theta}\\partial \\boldsymbol{\\theta}^\\top\\), evaluated at the maximum likelihood estimator \\(\\hat{\\boldsymbol{\\theta}}\\). By definition, the maximum likelihood estimator solves the score equation, i.e. \\(U(\\hat{\\boldsymbol{\\theta}})=\\boldsymbol{0}_p\\). If the maximum likelihood estimator is not available in closed-form, its solution is found numerically and this property can be used to verify that the optimization routine has converged or for gradient-based maximization algorithms. 1.1.1 Likelihoods There are four basic likelihoods for univariate extremes: the likelihood of the generalized extreme value (GEV) distribution for block maxima, the likelihood for the generalized Pareto distribution and that of the non-homogeneous Poisson process (NHPP) for exceedances above a threshold \\(u\\) and lastly the likelihood of the \\(r\\)-largest observations. The generalized extreme value (GEV) distribution with location parameter \\(\\mu \\in \\mathbb{R}\\), scale parameter \\(\\sigma \\in \\mathbb{R}_{+}\\) and shape parameter \\(\\xi \\in \\mathbb{R}\\) is \\[\\begin{align*} G(x) = \\begin{cases} \\exp\\left\\{-\\left(1+\\xi \\frac{x-\\mu}{\\sigma}\\right)^{-1/\\xi}\\right\\}, &amp; \\xi \\neq 0,\\\\ \\exp \\left\\{ -\\exp \\left(-\\frac{x-\\mu}{\\sigma}\\right)\\right\\},&amp; \\xi = 0, \\end{cases} \\end{align*}\\] defined on \\(\\{x \\in \\mathbb{R}: \\xi(x-\\mu)/\\sigma &gt; -1\\}\\) where \\(x_{+} = \\max\\{0, x\\}\\). The case \\(\\xi=0\\) is commonly known as the Gumbel distribution. We denote the distribution by \\(\\mathsf{GEV}(\\mu, \\sigma, \\xi)\\). The generalized Pareto (GP) distribution with scale \\(\\sigma \\in \\mathbb{R}_{+}\\) and shape \\(\\xi \\in \\mathbb{R}\\) is \\[\\begin{align*} G(x) = \\begin{cases} 1-\\left(1+\\xi \\frac{x}{\\sigma}\\right)_{+}^{-1/\\xi}, &amp; \\xi \\neq 0,\\\\ 1- \\exp \\left(-\\frac{x}{\\sigma}\\right),&amp; \\xi = 0. \\end{cases} \\end{align*}\\] The range of the generalized Pareto distribution is \\([0, -\\sigma/\\xi)\\) if \\(\\xi &lt; 0\\) and is \\(\\mathbb{R}_{+}\\) otherwise. We denote the distribution by \\(\\mathsf{GP}(\\sigma, \\xi)\\). Let \\(Y_{(1)} \\geq \\cdots \\geq Y_{(r)}\\) denote the \\(r\\) largest observations from a sample. The likelihood of the limiting distribution of the point process for the \\(r\\)-largest observations is, for \\(\\mu,\\xi\\in\\mathbb{R}, \\sigma&gt;0\\), \\[ \\ell(\\mu,\\sigma,\\xi; \\boldsymbol{y}) \\equiv -r\\log(\\sigma) - \\left(1+\\frac{1}{\\xi}\\right)\\sum_{j=1}^r \\log\\left(1 + \\xi\\frac{y_{(j)}-\\mu}{\\sigma}\\right)_{+} - \\left(1 + \\xi\\frac{y_{(r)}-\\mu}{\\sigma}\\right)^{-1/\\xi}_+. \\] This likelihood can be used to model the \\(r\\)-largest observations per block or threshold exceedances where the threshold is the \\(r\\)th order statistic Consider a sample of \\(N\\) observations, of which \\(n_u\\) exceed \\(u\\) and which we denote by \\(y_1, \\ldots, y_{n_u}\\). The likelihood associated to the limiting distribution of threshold exceedances is, for \\(\\mu, \\xi \\in \\mathbb{R}, \\sigma &gt;0\\), \\[\\begin{align} L(\\mu, \\sigma, \\xi; \\boldsymbol{y}) = \\exp \\left[ - c \\left\\{1+ \\xi \\left( \\frac{u-\\mu}{\\sigma}\\right)\\right\\}^{-1/\\xi}_{+}\\right] (c\\sigma)^{-n_u}\\prod_{i=1}^{n_u} \\left\\{1+\\xi\\left( \\frac{y_i-\\mu}{\\sigma}\\right)\\right\\}^{-1/\\xi-1}_{+},\\label{eq:ppp_lik} \\end{align}\\] where \\((\\cdot)_{+} = \\max\\{0, \\cdot\\}\\). The quantity \\(c\\) is a tuning parameter whose role is described in 7.5 of Coles (2001). If we take \\(c=N/m\\), the parameters of the point process likelihood correspond to those of the generalized extreme value distribution fitted to blocks of size \\(m\\). The NHPP likelihood includes a contribution for the fraction of points that exceeds the threshold, whereas the generalized Pareto is a conditional distribution, whose third parameter is the normalizing constant \\(\\zeta_u=\\Pr(Y&gt;u)\\). Since the latter has a Bernoulli and \\(\\zeta_u\\) is orthogonal to the pair \\((\\sigma, \\xi)\\), it is often omitted from further analyses and estimated as the proportion of samples above the threshold. 1.1.2 Test statistics and the profile likelihood The three main type of test statistics for likelihood-based inference are the Wald, score and likelihood ratio tests. The three main classes of statistics for testing a simple null hypothesis \\(\\mathrm{H}_0: \\boldsymbol{\\theta}=\\boldsymbol{\\theta}_0\\) against the alternative \\(\\mathrm{H}_a: \\boldsymbol{\\theta} \\neq \\boldsymbol{\\theta}_0\\) are the likelihood ratio, the score and the Wald statistics, defined respectively as \\[\\begin{align*} w &amp;= 2 \\left\\{ \\ell(\\hat{\\boldsymbol{\\theta}})-\\ell(\\boldsymbol{\\theta}_0)\\right\\},\\qquad \\\\w_{\\mathsf{score}} &amp;= U^\\top(\\boldsymbol{\\theta}_0)i^{-1}(\\boldsymbol{\\theta}_0)U(\\boldsymbol{\\theta}_0),\\qquad \\\\ w_{\\mathsf{wald}} &amp;= (\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}_0)^\\top i(\\boldsymbol{\\theta}_0)(\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{\\theta}_0), \\end{align*}\\] where \\(\\hat{\\boldsymbol{\\theta}}\\) is the maximum likelihood estimate under the alternative and \\(\\boldsymbol{\\theta}_0\\) is the null value of the parameter vector. The statistics \\(w, w_{\\mathsf{score}}, w_{\\mathsf{wald}}\\) are all first order equivalent and asymptotically follow a \\(\\chi^2_p\\) distribution, where \\(q\\) is the difference between \\(p\\) and the number of parameters under the null hypothesis. Under the conditions of the Neyman–Pearson theorem, the likelihood ratio test is most powerful test of the lot. The score statistic \\(w_{\\mathsf{score}}\\) only requires calculation of the score and information under \\(\\mathrm{H}_0\\), which can be useful in problems where calculations under the alternative are difficult to obtain. The Wald statistic \\(w_{\\mathsf{wald}}\\) is not parametrization-invariant and typically has poor coverage properties. Oftentimes, we are interested in a functional of the parameter vector \\(\\boldsymbol{\\theta}\\). The profile likelihood \\(\\ell_\\mathrm{p}\\), a function of \\(\\boldsymbol{\\psi}\\) alone, is obtained by maximizing the likelihood pointwise at each fixed value \\(\\boldsymbol{\\psi}=\\boldsymbol{\\psi}_0\\) over the nuisance vector \\(\\boldsymbol{\\lambda}_{\\psi_0}\\), \\[\\begin{align*} \\ell_\\mathrm{p}(\\boldsymbol{\\psi})=\\max_{\\boldsymbol{\\lambda}}\\ell(\\boldsymbol{\\psi}, \\boldsymbol{\\lambda})=\\ell(\\boldsymbol{\\psi}, \\hat{\\boldsymbol{\\lambda}}_{\\boldsymbol{\\psi}}). \\end{align*}\\] We denote the restricted maximum likelihood estimator \\(\\hat{\\boldsymbol{\\theta}}_\\psi= (\\psi, \\hat{\\lambda}_{\\psi})\\). We can define score and information in the usual fashion: for example, the observed profile information function is \\[j_\\mathrm{p}(\\boldsymbol{\\psi}) =-\\frac{\\partial \\ell_\\mathrm{p}(\\boldsymbol{\\psi})}{\\partial \\boldsymbol{\\psi}\\partial \\boldsymbol{\\psi}^\\top} = \\left\\{j^{\\boldsymbol{\\psi\\psi}}(\\boldsymbol{\\psi}, \\hat{\\boldsymbol{\\lambda}}_{\\boldsymbol{\\psi}})\\right\\}^{-1}. \\] The profile likelihood is not a genuine likelihood in the sense that it is not based on the density of a random variable. We can turn tests and their asymptotic distribution into confidence intervals. For the hypothesis \\(\\psi = \\psi_0\\), a \\((1-\\alpha)\\) confidence interval based on the profile likelihood ratio test is \\(\\{ \\psi: 2\\{\\ell(\\hat{\\theta}) - \\ell(\\hat{\\theta}_{\\psi})\\} \\leq \\chi^2_1(0.95)\\}\\). Two typical questions in extreme values are: given the intensity of an extreme event, what is its recurrence period? and what is a typical worst-case scenario over a given period of time? For the latter, suppose for simplicity that the daily observations are blocked into years, so that inference is based on \\(N\\) points for the \\(N\\) years during which the data were recorded. The return level is a quantile of the underlying distribution corresponding to an event of probability \\(p=1-1/T\\) for an annual maximum, which is interpreted as ``the level exceeded by an annual maximum on average every \\(T\\) years’’. If observations are independent and identically distributed, then we can approximate the probability that a return level is exceeded \\(l\\) times over a \\(T\\) year period using a binomial distribution with probability of success \\(1-1/T\\) and \\(T\\) trials. For \\(T\\) large, the return level is exceeded \\(l=0, 1, 2, 3, 4\\) times within any \\(T\\)-years period with approximate probabilities 36.8%, 36.8%, 18.4%, 6.1% and 1.5%. The probability that the maximum observation over \\(T\\) years is exceeded with a given probability is readily obtained from the distribution of the \\(T\\)-year maximum, leading (Cox, Isham, and Northrop 2002, 3(b)) to advocate its use over return levels, among other quantities of interest such as the number of times a threshold \\(u\\) will be exceeded in \\(T\\) years or the average number of years before a threshold \\(u\\) is exceeded. Quantiles, mean and return levels of \\(T\\)-maxima: consider the distribution \\(H(x) = G^T(x)\\) of the maximum of \\(T\\) independent and identically distributed generalized extreme value variates with parameters \\((\\mu, \\sigma, \\xi)\\) and distribution function \\(G\\). By max-stability, the parameters of \\(H(x)\\) are \\(\\mu_T=\\mu-\\sigma(1-T^\\xi)/\\xi\\) and \\(\\sigma_T=\\sigma T^\\xi\\) when \\(\\xi \\neq 0\\). We denote the expectation of the \\(T\\)-observation maximum by \\(\\mathfrak{e}_T\\), the \\(p\\) quantile of the \\(T\\)-observation maximum by \\(\\mathfrak{q}_p = H^{-1}(p)\\) and the associated return level by \\(z_{1/T} = G^{-1}(1-1/T)\\). Then, any of these three quantities can be written as \\[\\begin{align*} \\begin{cases} \\mu-\\frac{\\sigma}{\\xi}\\left\\{1-\\kappa_{\\xi}\\right\\}, &amp; \\xi &lt;1, \\xi \\neq 0, \\\\ \\mu+\\sigma\\kappa_0, &amp; \\xi =0, \\end{cases} \\end{align*}\\] where \\(\\kappa_{\\xi}=T^\\xi\\Gamma(1-\\xi)\\) for \\(\\mathfrak{e}_T\\), \\(\\kappa_{\\xi}=T^\\xi\\log(1/p)^{-\\xi}\\) for \\(\\mathfrak{q}_p\\) and \\(\\kappa_{\\xi}=\\left\\{-\\log\\left(1-{1}/{T}\\right)\\right\\}^{-\\xi}\\) for \\(z_{1/T}\\). In the Gumbel case, we have \\(\\kappa_0=\\log(T)+\\gamma_{e}\\) for \\(\\mathfrak{e}_T\\), \\(\\kappa_0=\\log(T)-\\log\\{-\\log(p)\\}\\) for \\(\\mathfrak{q}_p\\) and \\(\\kappa_0=-\\log\\{-\\log(1-1/T)\\}\\) for \\(z_{1/T}\\). 1.2 Numerical example This example illustrates some of the functions used in peaks-over-threshold analysis based on fitting a generalized Pareto distribution to threshold exceedances. We use the Venezuelian rainfall data, a time series of daily rainfall precipitations at Maiquetia airport in Venezuela, for the purpose of illustration. library(mev) data(&quot;maiquetia&quot;, package = &quot;mev&quot;) day &lt;- seq.Date(from = as.Date(&quot;1961-01-01&quot;), to = as.Date(&quot;1999-12-31&quot;), by = &quot;day&quot;) # Keep non-zero rainfall, exclude 1999 observations nzrain &lt;- maiquetia[substr(day, 3, 4) &lt; 99 &amp; maiquetia &gt; 0] gpdf &lt;- fit.gpd(nzrain, threshold = 20) print(gpdf) ## Method: Grimshaw ## Log-likelihood: -832.629 ## ## Threshold: 20 ## Number Above: 216 ## Proportion Above: 0.0604 ## ## Estimates ## scale shape ## 15.5800 0.1088 ## ## Standard Errors ## scale shape ## 1.60673 0.07785 ## ## Optimization Information ## Convergence: successful We will ignore temporal dependence and stationarity, but these should be considered. The first step in our analysis is to choose a threshold. For the time being, we set the latter to 20 and consider threshold selection in the next section. The default optimization routine for the generalized Pareto distribution is Grimshaw’s method, which profiles out the likelihood. The method has theoretical convergence guaranteesfor convergence. Because of non-regularity, the maximum likelihood estimator for \\(\\xi &lt; -1\\) does not solve the score equation and leads to infinite log-likelihood, hence the maximum returned lies on the boundary of the parameter space. The standard errors are based on the inverse observed information matrix and provided only if \\(\\xi&gt;-1/2\\). We can verify that our maximum likelihood estimate is indeed a maximum by checking if it solves the score equation if \\(\\hat{\\xi}&gt;-1\\). isTRUE(all.equal( gpd.score(gpdf$estimate, dat = gpdf$exceedances), c(0,0), tolerance = 1e-5)) ## [1] TRUE If the sample is small, maximum likelihood estimators are biased for the generalized Pareto distribution (the shape parameter is negatively biased, regardless of the true value for \\(\\xi\\)). Bias correction methods includes the modified score of Firth, but the default method is the implicit correction (subtract), which solves the implicit equation \\[\\begin{align} \\boldsymbol{\\tilde{\\theta}}=\\hat{\\boldsymbol{\\theta}}-\\boldsymbol{b}(\\tilde{\\boldsymbol{\\theta}}). \\label{eq:implbias} \\end{align}\\] The point estimate \\(\\boldsymbol{\\tilde{\\theta}}\\) is obtained numerically as the root of this nonlinear system of equations. In the present case, the sample size is large and hence the first-order correction, derived through asymptotic arguments from the generalized Pareto distribution likelihood, is small. Note that the bias correction requires \\(\\xi &gt; -1/3\\), since it is based on third-order cumulants of the distribution. gpdbcor &lt;- gpd.bcor(dat = gpdf$exceedances, par = gpdf$estimate) #print the differences between MLE and bias-corrected estimates gpdf$estimate - gpdbcor ## scale shape ## 0.1915284 -0.0118876 The package includes some default diagnostic plots (probability-probability plots and quantile-quantile plots), which include approximate confidence intervals based on order statistics. We can also get profile likelihood and profile-based confidence intervals for most quantities of interest (parameters of the generalized Pareto distribution, excepted shortfall, return levels, \\(N\\)-observation maxima mean and quantiles). The example below gives the estimated profile for the median of the centenial maximum distribution conditional on exceeding 15, along with 95% confidence intervals. plot(gpdf, which = 2) #Q-Q plot # Profile of median of maxima of 100 years profile &lt;- gpd.pll(param = &quot;Nquant&quot;, dat = nzrain, threshold = 15, N = length(nzrain)/(1998-1961)*100, q = 0.5) # 95% confidence intervals conf &lt;- confint(profile, print = TRUE) ## Point estimate for the parameter of interest psi: ## Maximum likelihood : 320.622 ## ## Confidence intervals, levels : 0.025 0.975 ## Wald intervals : 118.72 522.524 ## Profile likelihood : 177.214 741.869 1.3 Your turn Simulate 200 observations from the \\(r\\)-largest likelihood using rrlarg with shape parameter \\(\\xi=-0.2\\) and \\(r=5\\). Test the hypothesis \\(\\mathrm{H}_0: \\xi = \\xi_0\\) using a score test and derive a 90% confidence interval for \\(\\xi\\). You can obtain the maximum likelihood estimator by calling fit.rlarg and the score and information matrix are implemented under rlarg.score and rlarg.infomat. Recall that the score statistic \\(w_{\\mathsf{score}} \\equiv U(\\hat{\\boldsymbol{\\theta}}_{\\xi_0})^\\top i^{-1}(\\hat{\\boldsymbol{\\theta}}_{\\xi_0})U( \\hat{\\boldsymbol{\\theta}}_{\\xi_0}) \\sim \\chi^2_1.\\) References "],
["threshold-selection.html", "2 Threshold selection 2.1 Threshold stability plots 2.2 Wadsworth’s diagnostics: white noise process and simultaneous threshold stability plots 2.3 Changepoint tests based on penultimate approximation 2.4 Extended generalized Pareto 2.5 Robust selection 2.6 Your turn", " 2 Threshold selection The generalized Pareto (GP) distribution with scale \\(\\sigma \\in \\mathbb{R}_{+}\\) and shape \\(\\xi \\in \\mathbb{R}\\) is \\[\\begin{align*} G(x) = \\begin{cases} 1-\\left\\{1+\\xi \\left(\\frac{x}{\\sigma}\\right)\\right\\}_{+}^{-1/\\xi}, &amp; \\xi \\neq 0, \\\\ 1-\\exp \\left(-{x}{\\sigma}\\right),&amp; \\xi = 0. \\end{cases} \\end{align*}\\] The range of the generalized Pareto distribution is \\([0, -\\sigma/\\xi)\\) if \\(\\xi &lt; 0\\) and \\(\\mathbb{R}_{+}\\) otherwise. Why is the generalized Pareto distribution so central to peaks-over-threshold analysis? The conditional distribution of exceedances over a threshold \\(u &lt; x^*\\) converges to a generalized Pareto distribution as \\(u\\) converges to the endpoint of the distribution, \\[\\begin{align*} \\lim_{u \\to x^*} \\frac{1-F(xa_u+u)}{1-F(u)} = 1-H(x), \\end{align*}\\] where \\(H(x)\\) is the distribution function of \\(\\mathsf{GP}(1, \\xi)\\). If \\(X \\sim \\mathsf{GP}(\\sigma, \\xi)\\), straightforward calculations show that \\(X-u \\mid X&gt;u \\sim \\mathsf{GP}(\\sigma + \\xi u, \\xi)\\) for any \\(u \\in \\mathbb{R}\\) such that \\(\\sigma+\\xi u&gt;0\\), meaning that conditional exceedances above a threshold \\(u\\) also follow a generalized Pareto distribution. This property is termed threshold-stability. The limiting distribution of threshold exceedances is generalized Pareto as \\(u \\to x^*\\) but, in practice, we must choose a finite threshold in order to have enough exceedances to draw inference. Since the scaling constant \\(a_u\\) is unknown, we have \\(X \\mid X &gt; u \\stackrel{\\cdot}{\\sim} \\mathsf{GP}(\\sigma_u, \\xi)\\). The term \\(1-F(u)\\) in the denominator is the fraction of points exceeding the threshold, which has a binomial distribution. Threshold selection is subtle and it is common to select a high percentile of the data, say the 95% value, as the threshold, even if this is asymptotically incorrect, as in this case \\(k/n \\nrightarrow 0\\) as \\(n \\to \\infty\\). Most approaches for threshold selection rely on properties of the generalized Pareto distribution (moments, threshold-stability) to determine a region within which the asymptotic distribution fits the data well and the parameter estimates are stable. Below, we focus on recent graphical selection tools for likelihood based inference; mixture models are reviewed in Scarrott and MacDonald (2012) and are available in the package evmix. 2.1 Threshold stability plots Consider a sequence of ordered candidate thresholds \\(u_1 &lt; \\cdots &lt; u_k\\); one of the most widely used tools for threshold selection is the threshold-stability plots of Davison and Smith (1990). These show the point estimates of the shape \\(\\xi\\) and the modified scale \\(\\sigma_{u_i}-\\xi u_i\\), which should be constant for any threshold \\(u_{j} &gt;u_i\\) assuming that the generalized Pareto above \\(u_i\\) holds exactly. In addition to the point estimates, the asymptotic pointwise 95% Wald confidence intervals are displayed; the standard errors are obtained from the observed information matrix. While these are displayed by many packages, notably extRemes, ismev and evd , mev allows you to use profile likelihood based confidence intervals, which typically offer better coverage and capture some of the asymmetry of the distribution. The mev package functions for producing threshold stability plots are tstab.gpd and tstab.pp for respectively the generalized Pareto and Poisson process likelihoods. Parameter stability plots can be difficult to interpret because the confidence intervals are pointwise rather than simultaneous (each fit likewise uses an overlapping portion of the data). The plots also ignore changes in the estimated parameters due to the penultimate approximation. 2.2 Wadsworth’s diagnostics: white noise process and simultaneous threshold stability plots The problem with the threshold stability plots lies in the point-wise nature of the estimate. Assuming a superposition of \\(k\\) Poisson processes, Wadsworth (2016) derives the limiting distribution of the maximum likelihood estimators from the Poisson process for overlapping windows as the number of windows \\(k \\to \\infty\\) and \\(n_k \\to \\infty\\). The joint asymptotic Gaussian distribution allows Wadsworth (2016) to propose two additional diagnostics: a white noise sequence of differences in estimates of the shape, standardized to have unit variance. The variables \\(\\xi^*_i=(\\hat{\\xi}_{u_{i+1}}-\\hat{\\xi}_{u_i})/\\{(I^{-1}_{u_{i+1}}-I^{-1}_{u_{i}})_{\\xi,\\xi}^{1/2}\\}\\), where \\(I_{u_{i}}\\) is the Fisher information of the Poisson process likelihood for exceedances above \\(u_i\\), should form a white-noise sequence of independent variables centered around the origin; systematic deviations are indicative of inadequacy. To formally test the hypothesis, a likelihood ratio test can be used assuming a simple alternative, namely a single change point at threshold \\(u_j\\). The null hypothesis is \\(\\mathrm{H}_0: \\xi_i^* \\stackrel{\\mathrm{iid}}{\\sim}\\textsf{No}(0,1)\\) for \\(i=1, \\ldots, k-1\\) against the alternative \\(\\mathrm{H}_a: \\xi^*_i \\sim \\textsf{No}(\\beta, \\sigma) (i=1, \\ldots, j-1)\\) and \\(\\xi^*_i \\sim \\textsf{No}(0,1)\\) for \\(j, \\ldots, k-1\\). This alternative is motivated by results on model misspecification (White 1982), which suggest that the asymptotic distribution may still be Gaussian, but with a different mean and variance. This can be used to automate threshold selection, by picking the smallest threshold for which the \\(P\\)-value is above the level \\(\\alpha\\). The function W.diag returns diagnostics plots (for the likelihood ratio statistic path, the white noise process and threshold stability along with asymptotic simultaneous confidence intervals) for non-homogeneous Poisson process model and for the bivariate exponential and the over a sequence of thresholds, specified using q1, q2 and k. The argument M is a tuning parameter that can be chosen in a way such that the parameters of the non-homogeneous Poisson process likelihood coincide with those of the generalized extreme value distribution for blocks of size \\(m\\); see Coles (2001) to this effect. A main criticism of the proposals of Wadsworth (2016) is their lack of robustness. For the asymptotic result to be approximately valid, the number of thresholds must be large, which implicitly requires large samples for each superposed point process. Moreover, the estimated difference in Fisher information matrices often fails to be positive definite in practice. The procedure is highly sensitive to the choice of \\(k\\). Changing the set of thresholds \\(\\boldsymbol{u}\\) under consideration leads to potentially completely different parameter estimates being chosen by the automated procedure. 2.3 Changepoint tests based on penultimate approximation Let \\(F(x)\\) denote a thrice-differentiable distribution function with endpoint \\(x^*\\) and density \\(f(x)\\). Define the reciprocal hazard function \\(r(x) = \\{1-F(x)\\}/f(x)\\). The existence of the limit \\(\\xi_{\\infty} = \\lim_{n \\to \\infty} s&#39;\\{b_n\\}\\) is necessary and sufficient for convergence to an extreme value distribution and Smith (1987) shows that there exists \\(y\\) such that \\[\\begin{align*} \\frac{1-F\\{u+xr(u)\\}}{1-F(u)} = \\left\\{1+xr&#39;(y)\\right\\}_{+}^{-1/r&#39;(y)}, \\qquad u &lt; y &lt; u+xr(u), \\end{align*}\\] unless \\(r&#39;(x)\\) is constant. The penultimate shape parameter for the generalized Pareto distribution is \\(r&#39;(u)\\), but the true shape parameter lies between \\(r&#39;(u)\\) and \\(\\xi_{\\infty}\\). When we fit the limiting parametric models to finite samples, maximum likelihood estimates of the shape parameter will be closer to their penultimate counterparts than to the limiting value and we can expect them to vary as we increase the threshold. Northrop and Coleman (2014) adapt the idea of Wadsworth and Tawn (2012) and fit a generalized Pareto model with piecewise constant shape to \\(k\\) different thresholds; continuity constraints at the thresholds impose \\(k-1\\) restrictions on scale parameters, so the model only has \\(k+1\\) parameters. A score test can be used to test the hypothesis of equal shape and it only requires evaluation of the model under the null hypothesis that a generalized Pareto distribution is valid above all thresholds. A diagnostic plot is obtained by plotting \\(P\\)-values against threshold. One can then choose to take, e.g., (a) the lowest threshold at which the \\(P\\)-value is non-significant, or (b) the lowest threshold at which the \\(P\\)-values for all higher thresholds are non-significant: under the null hypothesis, there is an \\(\\alpha\\%\\) probability of rejection at any given threshold. The function NC.diag computes the \\(P\\)-value of the score test as a function of the threshold. 2.4 Extended generalized Pareto Papastathopoulos and Tawn (2013) propose three extended generalized Pareto distributions: for example, the third extended generalized Pareto model has distribution function \\(\\{1-(1+\\xi x/\\sigma)^{-1/\\xi}_{+}\\}^{\\kappa}\\) for \\(x &gt;0\\) and \\(\\kappa &gt; 0\\). Each family reduces to the generalized Pareto when the additional parameter \\(\\kappa=1\\) and share the same tail index \\(\\xi\\), the extended generalized Pareto provide more flexibility for modelling departures from the limiting form. Standard parameter stability plots can be used to find a region in which \\(\\kappa \\approx 1\\) and the shape parameter stabilizes. The additional parameter gives flexibility for modelling departures from the limiting distribution and the hope is one can fit to exceedances over lower threshold and increase the number of points to which the distribution is fitted. The stability plots, obtained through tstab.egp, suffer from the same caveats as classical diagnostics. 2.5 Robust selection The extreme value distributions have unbounded influence functions and outliers can strongly affect the estimate of the shape. Dupuis (1999) proposes an optimal \\(B\\)-robust estimator of the generalized Pareto parameters. Points that are outlying or for which the fit is poor are downweighted; if the weights for the largest observations are very low, this suggests that the threshold is too low. While there is no guarantee that observations that were simulated from a generalized Pareto distributed would not be downweighted, systematic downweighting of the largest exceedances may be indicative of poor fit. We are now ready to go back to the Maiquetia data and try to select a reasonable threshold. To this effect, we select a sequence of candidate thresholds and run the procedures described above. data(&quot;maiquetia&quot;, package = &quot;mev&quot;) day &lt;- seq.Date(from = as.Date(&quot;1961-01-01&quot;), to = as.Date(&quot;1999-12-31&quot;), by = &quot;day&quot;) # Keep non-zero rainfall, exclude 1999 observations nzrain &lt;- maiquetia[substr(day, 3, 4) &lt; 99 &amp; maiquetia &gt; 0] thcan &lt;- quantile(nzrain, seq(0.8, 0.99, length.out = 25)) tstab.gpd(dat = nzrain, thresh = thcan, method = &quot;profile&quot;) tstab.egp(xdat = nzrain, thresh = thcan, model = &quot;egp2&quot;) What do you notice on the parameter stability plots? We can plot simultaneous parameter statibility plots for the shape, as well as the white-noise sequence. The W.diag function, which implements the tests of Wadsworth (2016), returns the white noise process and calculates by simulation the null distribution, selecting the lowest threshold at which the hypothesis of changepoint is not rejected for all higher thresholds. For the Maiquetia rainfall, a threshold around 15.15mm is returned. fWdiag &lt;- W.diag(xdat = nzrain, model = &quot;nhpp&quot;, u = thcan, plots = c(&quot;WN&quot;, &quot;PS&quot;)) Another option is the Northrop and Coleman (2014) penultimate test, which considers a sequence of threshold and tests whether a single shape fits the data against the alternative where the shape parameter differs at each candidate threshold following a step function. The NC.diag function plots the \\(p\\)-value path against the candidate thresholds; the top axis also displays the number of threshold exceedances. The extended generalized Pareto distribution can also be used to produce threshold stability plot with 95% confidence intervals. The parameter \\(\\kappa=1\\) if the model is generalized Pareto. Although there are three potential families, the family egp2 exhibits the fastest converence for fNCdiag &lt;- NC.diag(x = nzrain, u = thcan) 2.6 Your turn Try using the threshold diagnostic tests on your favourite data. For example, you can use the infamous nidd river data. Which threshold would you choose based on the conclusions of the test? Compare the inference obtained by using the different threshold (by comparing, for example, return levels or quantiles of a distribution of interest). Are your results sensitive to the choice of threshold? References "],
["penultimate-approximation.html", "3 Penultimate approximation 3.1 Your turn", " 3 Penultimate approximation The generalized extreme value (GEV) distribution arises as the non-degenerate limiting distribution of maxima. A key property of the generalized extreme value distribution is max-stability, meaning that the distribution of maxima of \\(T \\in \\mathbb{N}\\) is the same up to a location-scale transform: if \\(F \\sim \\mathsf{GEV}\\), then there exist location and scale constants \\(b \\in \\mathbb{R}\\) and \\(a&gt;0\\) such that \\(F^T(ax+b) = F(x)\\). The parameters of the new distribution are easily derived: if \\(X_i \\stackrel{\\mathrm{iid}}{\\sim}\\mathsf{GEV}(\\mu,\\sigma, \\xi)\\), then \\(\\max\\{X_1, \\ldots, X_T\\} \\sim \\mathsf{GEV}(\\mu_T, \\sigma_T, \\xi)\\) with \\(\\mu_T = \\mu - \\sigma(1-T^\\xi)/\\xi\\) and \\(\\sigma_T = \\sigma T^\\xi\\) for \\(\\xi \\neq 0\\), or \\(\\mu_T = \\mu +\\sigma \\log(T)\\) and \\(\\sigma_T = \\sigma\\) if \\(\\xi=0\\). In practice, the GEV is fitted to data that are partitioned into \\(k\\) blocks of sizes \\(m\\), assumed eequal for simplicity, so that \\(n = km\\). The quality of the generalized extreme value approximation increases with the block size \\(m\\). For fixed block sizes, the estimated shape parameter generally differs from its asymptotic counterpart and this discrepancy usually introduces bias if one extrapolates far beyond the range of the observed data. Let \\(F(x)\\) denote a thrice-differentiable distribution function with endpoint \\(x^*\\) and density \\(f(x)\\) and define \\(s(x)=-F(x)\\log\\{F(x)\\}/f(x)\\). The existence of the limit \\(\\xi_{\\infty} = \\lim_{n \\to \\infty} s&#39;\\{b_n\\}\\) is necessary and sufficient for convergence to an extreme value distribution \\(G\\), \\[\\begin{align*} \\lim_{n \\to \\infty} F^n(a_nx+b_n) = \\exp\\left\\{-(1+\\xi_\\infty x)^{-1/\\xi_\\infty}\\right\\}=G(x). \\end{align*}\\] Smith (1987) shows that, for any \\(x \\in \\{y:1+\\xi_\\infty y &gt;0\\}\\), there exists \\(z\\) such that \\[\\begin{align*} \\frac{-\\log[F\\{v+xs(v)\\}]}{-\\log\\{F(v)\\}} = \\left\\{1+xs&#39;(z)\\right\\}^{-1/s&#39;(z)}, \\qquad v &lt; z &lt; v+xs(v). \\end{align*}\\] For each \\(n \\geq 1\\), setting \\(v=b_n\\) and \\(a_n=s(b_n)\\) yields \\[\\begin{align*} F^n(a_nx+b_n)=\\exp\\left[-\\left\\{1+s&#39;(z)x\\right\\}^{-1/s&#39;(z)}\\right] + \\mathrm{O}(n^{-1}) \\end{align*}\\] for \\(z \\in [\\min(a_nx+b_n, b_n), \\max(a_nx+b_n, b_n)]\\), depending on the support of \\(F\\). The ultimate approximation replaces \\(s&#39;(z)\\) by \\(s&#39;(x^*)=\\xi_{\\infty}\\), whereas Smith (1987) suggests instead suggests taking \\(s&#39;(b_n)\\), which is closer to \\(s&#39;(a_nx+b_n)\\) than is \\(s&#39;(x^*)\\). The maximum likelihood estimate of the shape should change as the threshold or the block size increases. Consider for simplicity yearly maxima arising from blocking \\(m=n_y\\) observations per year. We are interested in the distribution of the maximum of \\(N\\) years and thus the target has roughly shape \\(\\xi_{Nn_y}\\), but our estimates will instead give approximately \\(\\xi_{n_y}\\); an extrapolation error arises from this mismatch between the shape values, which would be constant if the observations were truly max-stable. The curvature of \\(s&#39;\\) determines how stable the estimates of \\(\\xi_t\\) are when the extrapolation window increases. We illustrate the previous discussion by looking at block maximum from a log-normal distribution. library(mev) set.seed(123) x &lt;- seq(1, 30, length=200) fitted &lt;- t(replicate(n = 1000, fit.gev(apply(matrix(rlnorm(30*100), ncol= 30), 1, max), method = &quot;BFGS&quot;)$est)) penult.bm.lnorm &lt;- mev::smith.penult(model = &quot;bm&quot;, m = (m &lt;- 30), family = &quot;lnorm&quot;) par(mfrow = c(1,2), mar = c(5,5,1,1)) hist(fitted[,3], probability = TRUE, breaks = 20, xlab = &quot;Estimated shape&quot;, main = &quot;&quot;) segments(y0 = -0.5, y1 = 0, x0 = penult.bm.lnorm$shape, col = &quot;red&quot;, lwd = 2) segments(y0 = -0.5, y1 = 0, x0 = 0, col = &quot;black&quot;, lwd = 2) p30 &lt;- penult.bm.lnorm x = seq(0,100, length = 400) N &lt;- 1000; N30 = N/30 plot(x, N*exp((N-1)*plnorm(x, log.p=TRUE))* dlnorm(x), type=&quot;l&quot;, bty = &quot;l&quot;, ylim = c(0,0.1), xlab=&quot;x&quot;, ylab=&quot;Density&quot;) # Get parameters of maximum of N GEV maxstabp &lt;- function(loc, scale, shape, N){ if(!isTRUE(all.equal(shape, 0, check.attributes = FALSE))){ mut &lt;- loc - scale*(1-N^shape)/shape sigmat = scale*N^shape return(c(loc = mut, scale = sigmat, shape = shape)) } else{ mut &lt;- loc + scale*log(N) return(c(loc = mut, scale = scale, shape = shape)) } } p30e &lt;- maxstabp(loc = p30$loc, scale = p30$scale, shape = p30$shape, N = N30) lines(x, evd::dgev(x, loc = p30e[&#39;loc&#39;], scale = p30e[&#39;scale&#39;], shape = p30e[&#39;shape&#39;]), col = &quot;red&quot;, lwd = 2, lty = 2) p30l &lt;- maxstabp(loc = p30$loc, scale = p30$scale, shape = 0, N = N30) lines(x, evd::dgev(x, loc = p30e[&#39;loc&#39;], scale = p30l[&#39;scale&#39;], shape = 0), col = 1, lty=3, lwd=1) legend(x = &quot;topright&quot;, legend = c(&quot;exact&quot;,&quot;ultimate&quot;, &quot;penultimate&quot;), col = c(1,1,2), lwd = c(2,2,1), bty = &quot;n&quot;, lty = c(1,3,2)) The left panel illustrates how maximum likelihood estimates of the parameters for repeated samples from a log-normal distribution are closer to the penultimate approximation than to the limiting tail index \\(\\xi_{\\infty}=0\\). Using max-stability, we could get an estimate of the distribution of the maximum of \\(Nn_y\\) observations. In this case, the penultimate shape for \\(m=1000\\) is approximately \\(\\xi_{1000}=0.215\\), compared to \\(\\xi_{30} \\approx 0.284\\), which is far from the limiting value \\(\\xi_{\\infty}=0\\). The extrapolated density estimate is based on \\(F_1^{1000/30}\\), the generalized extreme value density associated to the distribution function of the penultimate approximation \\(F_1^{1000/30}\\), with \\(F_1 \\sim\\mathsf{GEV}(a_{30}, b_{30}, \\xi_{30})\\). The penultimate approximation \\(F_2\\) is more accurate than the ultimate approximation \\(\\mathsf{GEV}(a_{30}, b_{30}, \\xi_\\infty)\\), which is too short tailed. The function smith.penult computes the penultimate approximation for parameter models in R for the distribution assuming a model family with arguments dfamily, qfamily and pfamily exist. It returns the penultimate parameters for a given block size or quantile in case of threshold models. These functions are particularly useful for simulation studies in which one wants to obtain an approximation for \\(F^N\\) for large \\(N\\), to get an approximation to the asymptotic distribution of a test statistic for finite \\(N\\); the \\(\\mathsf{GEV}\\) penultimate approximation is more numerically stable for certain models and its moments and cumulants are easily derived. The following plot illustrates the penultimate shape parameter for the normal distribution based on threshold exceedances. In finite samples, the upper tail is bounded, but the quality of the approximation by the Gumbel distribution increases as \\(u \\to \\infty\\), albeit very slowly. penult &lt;- smith.penult(family = &quot;norm&quot;, method = &quot;pot&quot;, u = qnorm(u &lt;- seq(0.8, 0.9999, by = 0.0001))) plot(u, penult$shape, bty = &quot;l&quot;, type=&#39;l&#39;, xlab=&#39;Quantile&#39;, ylab=&#39;Penultimate shape&#39;) While the above discussion has been restricted to block maxima and the generalized extreme value distribution, similar results exists for peaks over threshold using a generalized Pareto distribution. For threshold exceedances, we start with the reciprocal hazard \\(r(x) = \\{1-F(x)\\}/f(x)\\) in place of \\(s(x)\\). The normalizing constants are \\((u, r(u))\\) and the penultimate shape parameter for the generalized Pareto distribution is \\(r&#39;(u)\\). We can relate threshold exceedances to the distribution of maxima as follows: suppose a generalized Pareto distribution \\(F\\) is fitted to threshold exceedances above a threshold \\(u\\), with parameters \\((\\zeta_u, \\sigma_u, \\xi)\\), where \\(\\zeta_u\\) denotes the unknown proportion of points above the threshold \\(u\\). If there are \\(n_y\\) observations per year on average, then the \\(T\\)-year return level is \\(z_{1/T}=u+\\sigma_u/\\xi\\{(Tn_y\\zeta_u)^\\xi-1\\}\\). We take \\(F^{\\zeta_uTn_y}\\) as the approximation to the \\(T\\)-year maxima distribution, conditional on exceeding \\(u\\). 3.1 Your turn Sample observations from the standard Gaussian distribution \\(\\mathsf{No}(0,1)\\) for \\(m = 25, 50, 100, 1000\\) and \\(k=1000\\) replications. Fit a generalized extreme value distribution to block maxima of size \\(m\\). Obtain an estimate of the distribution of \\(m = 1000000\\) observations from a standard Gaussian using max-stability (the functions fit.gev and gev.mle can be useful). Plot the density of the resulting approximations and compare them to the density of the true distribution \\(N\\phi(x)\\Phi^{N-1}(x)\\). Superpose the density of the penultimate approximation for \\(m = 1000000\\) obtained using the Smith penultimate approximation (smith.penult). References "],
["simulation.html", "4 Simulation 4.1 Max-stable processes 4.2 \\(\\mathcal{R}\\)-Pareto processes and generalizations 4.3 Algorithms 4.4 Extremal coefficient 4.5 Generalized \\(\\mathcal{R}\\)-Pareto processes for functional threshold exceedances", " 4 Simulation 4.1 Max-stable processes Let \\(\\mathcal{S}\\) be a compact subset of \\(\\mathbb{R}^d\\) The de Haan spectral representation of simple max-stable processes is \\[Z(\\boldsymbol{s}) = \\max_{i \\in \\mathbb{N}} \\zeta_i W_i(\\boldsymbol{s}), \\qquad \\boldsymbol{s} \\in \\mathcal{S},\\] where \\(\\boldsymbol{W}\\) is a stochastic process satisfying \\(\\mathrm{E}\\{W(\\boldsymbol{s})_{+}\\}=1\\) for any \\(\\boldsymbol{s} \\in \\mathcal{S}\\) and \\(\\{\\zeta_i\\}_{i \\in \\mathbb{N}}\\) is a Poisson point process with intensity function \\(\\zeta^{-2} \\mathrm{d} \\zeta\\). Max-stable processes arise as the pointwise maximum of an infinite collection of random functions \\(\\varphi_i = \\zeta_i \\boldsymbol{W}_i\\). The distribution function of a \\(D\\)-dimensional max-stable vector can be defined in terms of a measure \\(\\Lambda\\), termed exponent measure, defined on \\(\\mathbb{R}^D \\setminus\\{\\boldsymbol{0}_D\\}\\) viz. \\[\\Pr(\\boldsymbol{Z} \\leq \\boldsymbol{z}) = \\exp\\{\\Lambda([-\\boldsymbol{\\infty}, \\boldsymbol{z}]^\\mathrm{c})\\}, \\boldsymbol{z} &gt; \\boldsymbol{0}_D.\\] It follows from max-stability that the exponent measure is homogeneous of order \\(-1\\). 4.2 \\(\\mathcal{R}\\)-Pareto processes and generalizations Let \\(\\mathcal{F}^+\\) denote the set of continuous non-negative non-null functions. A risk functional \\(\\mathcal{R}:\\mathcal{F}^+ \\to [0, \\infty)\\) is a continuous functional which is homogeneous of order \\(1\\), i.e., \\(\\mathcal{R}(tf) = t\\mathcal{R}(f)\\) for \\(f \\in \\mathcal{F}^+\\) and \\(t&gt;0\\). A \\(\\mathcal{R}\\)-Pareto process admits the stochastic representation (Ferreira and Haan 2014, @Dombry/Ribatet:2015) \\[\\begin{align} \\mathsf{X} = P \\frac{\\mathsf{S}}{\\mathcal{R}(\\mathsf{S})}, \\end{align}\\] for \\(P\\) a unit Pareto random variable with survival function \\(\\Pr(P&gt;x) = x^{-\\alpha}\\) for \\(x \\geq 1\\) and a stochastic process \\(\\mathsf{S}\\), independent of \\(P\\), with sample paths in \\(\\mathbb{S}_{\\mathrm{ang}} = \\{f \\in \\mathcal{F}^+ \\setminus \\mathcal{C}_{\\mathcal{R}}: \\|f\\|_{\\mathrm{ang}} = 1\\}\\) and where \\(\\mathcal{C}_{\\mathcal{R}} = \\{g \\in \\mathcal{F}^+: \\mathcal{R}(x)=0\\}\\) and \\(\\|\\cdot\\|_{\\mathrm{ang}}\\) is a norm. \\(\\mathcal{R}\\)-Pareto processes come about when looking at the limit distribution of \\(\\mathcal{R}\\)-exceedances, i.e. \\(\\{f: \\mathcal{R}(f) &gt;u\\}\\) for a large threshold \\(u\\), assuming unit Frechet margins. This can be achieved in practice by standardizing the margins, but with the drawback that the exceedances are defined on the transformed scale. Fondeville (2018) lifts this assumption and define generalized \\(\\mathcal{R}\\)-Pareto processes, allowing for different location and scale marginal parameters (but with a common shape parameter). The risk region in which extreme observations lie is \\[\\begin{align*} \\mathcal{A}_u= \\left\\{f \\in \\mathcal{F}^+ \\setminus \\mathcal{C}_{\\mathcal{R}}: \\mathcal{R}\\left( \\boldsymbol{\\tau} \\frac{f^\\xi-1}{\\xi}+\\boldsymbol{\\eta}\\right) \\geq u\\right\\}, \\end{align*}\\] where \\(\\boldsymbol{\\eta}\\) is a location function and \\(\\boldsymbol{\\tau}\\) is a scale function. The probability measure of \\(\\mathsf{Z}^*\\) over \\(\\mathcal{A}_u\\) is \\(\\Lambda(\\cdot)/\\Lambda\\{\\mathcal{A}_u\\}\\). The stochastic representation of the generalized \\(\\mathcal{R}\\)-Pareto vector is \\[\\begin{align} Z(\\boldsymbol{s}) = \\tau(\\boldsymbol{s}) \\frac{X(\\boldsymbol{s})^{\\xi}-1}{\\xi}+ \\eta(\\boldsymbol{s}), \\end{align}\\] where \\(X(\\boldsymbol{s})\\) is a \\(\\mathcal{R}\\)-Pareto process. When \\(\\mathcal{R}\\) is \\(\\max\\), the finite-dimensional distributions are multivariate generalized Pareto (Rootzén, Segers, and Wadsworth 2018). 4.3 Algorithms The original goal of the mev package was to implement the algorithm of Dombry, Engelke, and Oesting (2016), in order to perform exact simulation from max-stable vectors by sampling each extremal function in turn. Since then, other algorithms for simulating from standard \\(\\mathcal{R}\\)-Pareto processes and generalized \\(\\mathcal{R}\\)-Pareto processes have been added to the package. These are based on accept-reject method or composition sampling and can be slow in high dimensions. Note that packages such as RandomFields and SpatialExtremes are tailored for spatial processes and include more efficient algorithms to sample the (log-)Gaussian components appearing in the Brown–Resnick and extremal Student models, which makes them typically faster. The vignette provides details about the parametrization of the models implemented in mev. The various functions of interest for simulation include rmev, which is the workshorse for simulating max-stable random vectors; rmevspec returns samples from the angular distribution defined with respect to the \\(l_1\\) norm; rparp for simulating \\(\\mathcal{R}\\)-Pareto processes through accept-reject; rparpcs for simulating \\(\\mathcal{R}\\)-Pareto processes using composition sampling, e.g. (???) rgparp for simulating generalized \\(\\mathcal{R}\\)-Pareto processes using accept-reject. This approach is only available for Brown–Resnick and extremal Student processes and is more computationally intensive since it requires computing numerically mixture weights. However, these need only be computed once and this approach is more efficient for simlating large samples, particularly in cases where the acceptance rate is very low (notably if \\(R(\\boldsymbol{X})=\\min_{j=1}^DX_j\\)). Let’s start by simulating from a simple max-stable random vector from the negative logistic model. The margins are unit Frechet, and we can verify this by pooling the observations and fitting a generalized extreme value distribution to the sample. The parameter estimates are correct, and a probability-probability plot shows that the . If \\(\\boldsymbol{Z}\\) is a max-stable vector of size \\(D\\) with standard Gumbel margins, with distribution function \\(G(\\boldsymbol{z}) = \\exp[-V\\{\\exp(\\boldsymbol{z})\\}]\\), then for any subset of size \\(J \\subseteq\\{1, \\ldots, D\\}\\), the variable \\(Z_J=\\max_{j \\in J} Z(\\boldsymbol{s}_j)\\) has distribution function \\(H_J(z) = \\exp\\{-\\exp(z-\\mu_{J})\\}\\), where \\(\\mu_J=\\log\\{V_J(\\boldsymbol{1})_{|J|})\\}\\) and \\(0 \\leq \\mu_J \\leq \\log(|J|)\\). This follows from the homogeneity of the exponent measure. Gabda et al. (2012) propose a probability-probability plot based on fitting \\(\\mu_J\\) through maximum likelihood with the parameter constraints for each set of \\(|J|\\) stations. The data are obtained by pooling replications of the max-stable field and selecting all subsets of size \\(|J|\\) if \\(\\binom{D}{|J|}\\) is small, or else a limited number of stations among those that display the higher dependence so as to maximize the power of the test (independence being a special case of max-stability). Uncertainty quantification is performed using a nonparametric bootstrap. The function maxstabtest implements this for the special case \\(J=D\\). library(mev) set.seed(0) samp &lt;- rmev(n = 1000, d = 5, param = 0.1, model = &quot;neglog&quot;) fgev &lt;- fit.gev(c(samp), show = FALSE) fgev$estimate ## loc scale shape ## 0.9954939 0.9919429 0.9867670 par(mfrow = c(1, 2)) # Test of max-stability maxstabtest(dat = samp) # Probability-probability plot plot(fgev, which = 1, main = &quot;&quot;) The likelihood of max-stable processes is untractable, so inference is typically performed using composite likelihood. The package evd includes routines for doing this in the bivariate setting (see fbvpot). Consider a collection of independent and identically distributed vectors \\(\\boldsymbol{X}\\) with continuous marginal distributions that are in the max-domain of attraction of a max-stable distribution with limit measure \\(\\Lambda\\). One can transform the observations \\(\\boldsymbol{X}_i\\) into pseudo-uniform samples using the empirical distribution function or else the semi-parametric estimator, \\[\\begin{align} \\check{F}_j(x) = \\begin{cases} \\mathrm{I}\\{i: y_{ij} \\leq x\\}/(n+1), &amp; x \\leq u_j,\\\\ 1-\\{1-\\tilde{F}_j(u_j)\\}\\left\\{1+\\hat{\\xi}\\left(\\frac{x-u_j}{\\hat{\\tau}_j}\\right)\\right\\}^{-1/\\hat{\\xi}_j}_{+}, &amp; x &gt;u_j. \\end{cases} \\end{align}\\] (see spunif). We can back-transform the pseudo-uniform samples \\(u_{ij} = \\check{F}_j(y_{ij})\\) to unit Pareto using the quantile transform, \\(z_{ij}^{\\mathrm{p}} = 1/1-(u_{ij})\\). If we map the random vector \\(\\boldsymbol{Z}^{\\mathrm{p}}\\) to pseudo-polar coordinates, \\(\\boldsymbol{Z}^{\\mathrm{p}} \\mapsto (R, \\boldsymbol{\\Omega})\\) from \\(\\mathbb{R}^D_{+} \\to (0, \\infty) \\times \\mathbb{S}_1\\), where \\(\\mathbb{S}_1=\\{ \\boldsymbol{\\omega} \\in \\mathbb{R}^D_{+}: \\|\\boldsymbol{\\omega}\\|_1=1\\}\\) is the \\(\\ell_1\\)-simplex. The measure \\(\\Lambda\\) factorizes as a product measure \\(\\Lambda(\\mathrm{d}\\boldsymbol{z}) = D \\zeta^{-2}\\mathrm{d}\\zeta \\rho_1(\\mathrm{d}\\boldsymbol{\\omega})\\) with angular measure \\(\\rho_1\\), a probability measure satisfying the moment constraint \\(D\\int_{\\mathbb{S}_1} \\omega_j \\rho_1(\\mathrm{d}\\boldsymbol{\\omega})=1\\) for \\(j=1, \\ldots, D\\); this moment constraint holds for any \\(\\rho_1\\), whereas it will be measure dependent if the radial measure is not \\(\\|\\cdot \\|_1\\) (Einmahl and Segers 2009). We can thus create an approximate sample \\(\\boldsymbol{\\omega}\\) from the angular measure \\(\\rho_1\\) and estimate its distribution non-parametrically as \\[\\hat{H}(w) = \\sum_{i=1}^N \\hat{p}_i\\mathrm{I}\\{\\omega_i \\leq w\\},\\] where the weights \\(\\hat{p}_i\\) are obtained as the solution of either the empirical or Euclidean likelihood problems with a mean constraint (Carvalho et al. 2013, @Einmahl:2009). Since the resulting spectral distribution is discrete (which is problematic in simulations), Carvalho et al. (2013) suggest fitting a Dirichlet kernel to observations, with parameters \\(\\nu \\boldsymbol{w}_i (i=1, \\ldots, D)\\) subject to the constraint \\(\\|\\nu\\boldsymbol{w}_{i}\\|_1=1\\). The’bandwidth’ tuning parameter \\(\\nu\\) is chosen via cross-validation. The function angmeasdir fits Euclidean likelihood and returns a list with the weights vector wts, the cross-validation parameter nu and a \\(n \\times D\\) matrix of parameters for the Dirichlet distribution. Alternatively, we can estimate the limiting spectral measure from the pseudo-observations enforcing the mean constraint using empirical likelihood, as proposed by Einmahl and Segers (2009). The function angmeas computes the weights associated to each observations. samp &lt;- rmev(n = 1000, d = 3, param = c(0.4,0.6,2.9, 0.1), model = &quot;sdir&quot;) taildep(samp, method = list(eta = &quot;betacop&quot;, chi = &quot;betacop&quot;)) nparangmeas &lt;- angmeas(samp, th = 0.5) # Plot the probability weights and compute the column mean plot(nparangmeas$wts, ylab = &quot;Weights&quot;); abline(h = 1/nrow(nparangmeas$ang)) colSums(nparangmeas$wts * nparangmeas$ang) ## [1] 0.3333333 0.3333333 dirangmeas &lt;- angmeasdir(samp, th = 0.5) 4.4 Extremal coefficient As in SpatialExtremes, we can use the nonparametric estimates of the extremal coefficient as goodness-of-fit diagnostic. The following code samples data coord &lt;- 10*cbind(runif(50), runif(50)) di &lt;- as.matrix(dist(coord)) dat &lt;- rmev(n = 1000, d = 100, param = 3, sigma = exp(-di/2), model = &#39;xstud&#39;) res &lt;- extcoef(dat = dat, coord = coord) # Extremal Student extremal coefficient function XT.extcoeffun &lt;- function(h, nu, corrfun, ...){ if(!is.function(corrfun)){ stop(&#39;Invalid function `corrfun`.&#39;) } h &lt;- unique(as.vector(h)) rhoh &lt;- sapply(h, corrfun, ...) cbind(h = h, extcoef = 2*pt(sqrt((nu+1)*(1-rhoh)/(1+rhoh)), nu+1)) } #This time, only one graph with theoretical extremal coef plot(res$dist, res$extcoef, ylim = c(1,2), pch = 20, ylab = &quot;extremal coefficient&quot;, xlab = &quot;distance&quot;) extcoefxt &lt;- XT.extcoeffun(seq(0, max(res$dist), by = 0.1), nu = 3, corrfun = function(x){exp(-x/2)}) lines(extcoefxt[,&#39;h&#39;], extcoefxt[,&#39;extcoef&#39;], type = &#39;l&#39;, col = &#39;blue&#39;, lwd = 2) 4.5 Generalized \\(\\mathcal{R}\\)-Pareto processes for functional threshold exceedances We simulate a spatial generalized \\(\\mathcal{R}\\)-Pareto process. lon &lt;- seq(650, 720, length = 50) lat &lt;- seq(215, 290, length = 50) # Create a grid grid &lt;- expand.grid(lon,lat) coord &lt;- as.matrix(grid) dianiso &lt;- distg(coord, 1.5, 0.5) sgrid &lt;- scale(grid, scale = FALSE) # Specify marginal parameters `loc` and `scale` over grid eta &lt;- 26 + 0.05*sgrid[,1] - 0.16*sgrid[,2] tau &lt;- 9 + 0.05*sgrid[,1] - 0.04*sgrid[,2] # Parameter matrix of Huesler--Reiss # associated to power variogram Lambda &lt;- ((dianiso/30)^0.7)/4 # Simulate generalized max-Pareto field above u=50 set.seed(345) simu1 &lt;- rgparp(n = 1, thresh = 50, shape = 0.1, riskf = &quot;max&quot;, scale = tau, loc = eta, sigma = Lambda, model = &quot;hr&quot;) # The same, but conditional on an exceedance at a site simu2 &lt;- rgparp(n = 1, thresh = 50, shape = 0.1, riskf = &quot;site&quot;, siteindex = 1225, scale = tau, loc = eta, sigma = Lambda, model = &quot;hr&quot;) #Plot the generalized max-Pareto field par(mfrow = c(1,2)) fields::quilt.plot(grid[,1], grid[,2], simu1, nx = 50, ny = 50) SpatialExtremes::swiss(add = TRUE) fields::quilt.plot(grid[,1], grid[,2], simu2, nx = 50, ny = 50) SpatialExtremes::swiss(add = TRUE) # Value at conditioning coordinate should be greater than 50 simu2[1225] ## [1] 84.81661 The code snippet below fits a Brown–Resnick model with power variogram to simulated data from the same model (based on more than one replicate). The parameters are estimated by minimizing the squared distance between empirical cloud of pairwise conditional probability of exceedance and the theoretical one. We include geometric anisotropy in the analysis. lon &lt;- seq(650, 720, length = 10) lat &lt;- seq(215, 290, length = 10) # Create a grid grid &lt;- expand.grid(lon,lat) coord &lt;- as.matrix(grid) dianiso &lt;- distg(coord, 1.5, 0.5) sgrid &lt;- scale(grid, scale = FALSE) # Specify marginal parameters `loc` and `scale` over grid eta &lt;- 26 + 0.05*sgrid[,1] - 0.16*sgrid[,2] tau &lt;- 9 + 0.05*sgrid[,1] - 0.04*sgrid[,2] # Parameter matrix of Huesler--Reiss # associated to power variogram Lambda &lt;- ((dianiso/30)^0.7)/4 # Simulate generalized max-Pareto field above u=50 set.seed(345) simu1 &lt;- rgparp(n = 1000, thresh = 50, shape = 0.1, riskf = &quot;max&quot;, scale = tau, loc = eta, sigma = Lambda, model = &quot;hr&quot;) extdat &lt;- extremo(dat = simu1, margp = 0.9, coord = coord, scale = 1.5, rho = 0.5, plot = TRUE) # Constrained optimization # Minimize distance between extremal coefficient from fitted variogram mindistpvario &lt;- function(par, emp, coord){ alpha &lt;- par[1]; if(!isTRUE(all(alpha &gt; 0, alpha &lt; 2))){return(1e10)} scale &lt;- par[2]; if(scale &lt;= 0){return(1e10)} a &lt;- par[3]; if(a&lt;1){return(1e10)} rho &lt;- par[4]; if(abs(rho) &gt;= pi/2){return(1e10)} semivariomat &lt;- mgp::power.vario(distg(coord, a, rho), alpha = alpha, scale = scale) sum((2*(1-pnorm(sqrt(semivariomat[lower.tri(semivariomat)]/2))) - emp)^2) } # constrained optimization for the parameters hin &lt;- function(par, ...){ c(1.99-par[1], -1e-5 + par[1], -1e-5 + par[2], par[3]-1, pi/2 - par[4], par[4]+pi/2) } opt &lt;- alabama::auglag(par = c(0.5, 30, 1.5, 0.5), hin = hin, control.optim = list(parscale = c(0.5, 30, 1.5, 0.5)), fn = function(par){ mindistpvario(par, emp = extdat[,&#39;prob&#39;], coord = coord)}) ## Min(hin): 0.49999 ## Outer iteration: 1 ## Min(hin): 0.49999 ## par: 0.5 30 1.5 0.5 ## fval = 77.37 ## ## Outer iteration: 2 ## Min(hin): 0.3992261 ## par: 0.668296 74.8862 1.39923 0.556808 ## fval = 6.52 ## ## Outer iteration: 3 ## Min(hin): 0.3992502 ## par: 0.668285 74.8855 1.39925 0.556807 ## fval = 6.52 ## stopifnot(opt$kkt1, opt$kkt2) # Plotting the extremogram in the deformed space distfa &lt;- distg(loc = coord, opt$par[3], opt$par[4]) plot(c(distfa[lower.tri(distfa)]), extdat[,2], pch = 20, col = scales::alpha(1,0.1), yaxs = &quot;i&quot;, xaxs = &quot;i&quot;, bty = &#39;l&#39;, xlab = &quot;distance&quot;, ylab= &quot;cond. prob. of exceedance&quot;, ylim = c(0,1)) lines(x = (distvec &lt;- seq(0,200, length = 1000)), col = 2, lwd = 2, 2*(1-pnorm(sqrt(power.vario(distvec, alpha = opt$par[1], scale = opt$par[2])/2)))) References "],
["references.html", "References", " References "]
]
